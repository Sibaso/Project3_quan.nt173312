{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CL_NMT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibaso/Project3_quan.nt173312/blob/main/CL_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbnb0yZ7xvUE",
        "outputId": "a3e048e0-4b3e-4961-c79d-f475ca5ab66c"
      },
      "source": [
        "!pip install pyvi\r\n",
        "!python -m spacy download en\r\n",
        "from pyvi import  ViTokenizer\r\n",
        "import spacy\r\n",
        "spacy_eng = spacy.load(\"en\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.19.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCFXdd5WTZPQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def data_to_json():\n",
        "    with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/basic/data.en', encoding='utf-8') as f:\n",
        "        english = [line for line in f.read().splitlines()]\n",
        "\n",
        "    with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/basic/data.vi', encoding='utf-8') as f:\n",
        "        vietnamese = [line for line in f.read().splitlines()]\n",
        "        \n",
        "    train_src, test_src, train_trg, test_trg = train_test_split(english, vietnamese, train_size=0.8, shuffle=True, random_state=1)\n",
        "    train_src, valid_src, train_trg, valid_trg = train_test_split(train_src, train_trg, train_size=0.8, shuffle=True, random_state=1)\n",
        "\n",
        "    df = pd.DataFrame({'eng': train_src, 'viet': train_trg}, columns=['eng', 'viet'])\n",
        "    df.to_json('/content/drive/My Drive/Project3/data/%s-%s_train.json' % ('ENG', 'VIET'), orient=\"records\", lines=True)\n",
        "\n",
        "    df = pd.DataFrame({'eng': test_src, 'viet': test_trg}, columns=['eng', 'viet'])\n",
        "    df.to_json('/content/drive/My Drive/Project3/data/%s-%s_test.json' % ('ENG', 'VIET'), orient=\"records\", lines=True)\n",
        "\n",
        "    df = pd.DataFrame({'eng': valid_src, 'viet': valid_trg}, columns=['eng', 'viet'])\n",
        "    df.to_json('/content/drive/My Drive/Project3/data/%s-%s_valid.json' % ('ENG', 'VIET'), orient=\"records\", lines=True)\n",
        "        \n",
        "data_to_json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypmM8_9Zw96m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from pyvi import  ViTokenizer\r\n",
        "import spacy\r\n",
        "spacy_eng = spacy.load(\"en\")\r\n",
        "\r\n",
        "def preprocess_eng(text):\r\n",
        "    text = ' '.join([tok.text for tok in spacy_eng.tokenizer(text)])\r\n",
        "    return ' '.join(re.split('\\W+', text))\r\n",
        "\r\n",
        "def preprocess_viet(text):\r\n",
        "    text = ViTokenizer.tokenize(text)\r\n",
        "    return ' '.join(re.split('\\W+', text))\r\n",
        "\r\n",
        "def data_to_json(name):\r\n",
        "    with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/%s.en'%name, encoding='utf-8') as f:\r\n",
        "        english = [preprocess_eng(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "    with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/%s.vi'%name, encoding='utf-8') as f:\r\n",
        "        vietnamese = [preprocess_viet(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "    df = pd.DataFrame({'eng': english, 'viet': vietnamese}, columns=['eng', 'viet'])\r\n",
        "    df.to_json('/content/drive/My Drive/Project3/data/%s-%s_big_%s.json' % ('eng', 'viet', name), orient=\"records\", lines=True)\r\n",
        "\r\n",
        "        \r\n",
        "data_to_json('tst')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqY_uGXeTZPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec3a69b-8112-4521-9dbd-7dcdaa08e5c7"
      },
      "source": [
        "import torchtext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset, Example\n",
        "from torchsummary import summary\n",
        "import math\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import os\n",
        "import time\n",
        "import nltk\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# device='cpu'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgtZJKg8ZAgl",
        "outputId": "b261ac7c-4371-4bc0-cb90-b178f5b9a497"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDhCXrck1At"
      },
      "source": [
        "EWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXkqBkxVTZPM"
      },
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.att_fc = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.att_fc(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.trb_fc1 = nn.Linear(embed_size, forward_expansion * embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.trb_fc2 = nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.trb_fc1(x)\n",
        "        forward = self.relu(forward)\n",
        "        forward = self.trb_fc2(forward)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.word_embedding = nn.ModuleDict()\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def add_task(self, src_vocab_size, src_token):\n",
        "        if src_token in self.word_embedding:\n",
        "          return\n",
        "        self.word_embedding[src_token] = nn.Embedding(src_vocab_size, self.embed_size).to(device)\n",
        "\n",
        "    def forward(self, x, mask, src_token):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding[src_token](x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.word_embedding = nn.ModuleDict()\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.ModuleDict()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def add_task(self, trg_vocab_size, trg_token):\n",
        "        if trg_token in self.word_embedding:\n",
        "          return\n",
        "        self.word_embedding[trg_token] = nn.Embedding(trg_vocab_size, self.embed_size).to(device)\n",
        "        self.fc_out[trg_token] = nn.Linear(self.embed_size, trg_vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask, trg_token):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
        "        x = self.dropout((self.word_embedding[trg_token](x) + self.position_embedding(positions)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out[trg_token](x)\n",
        "\n",
        "        return F.log_softmax(out, dim=-1)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def add_task(self, src_vocab_size, trg_vocab_size, src_token, trg_token):\n",
        "        self.encoder.add_task(src_vocab_size, src_token)\n",
        "        self.decoder.add_task(trg_vocab_size, trg_token)        \n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(device)\n",
        "\n",
        "    def forward(self, src, trg, src_token, trg_token):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask, src_token)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask, trg_token)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaRc0CdIzgdx"
      },
      "source": [
        "\n",
        "class EWC(object):\n",
        "    \"\"\" Class implementing the Elastic Weight Consolidation approach described in http://arxiv.org/abs/1612.00796 \"\"\"\n",
        "\n",
        "    def __init__(self,model, lamb=1000, nepochs=100,sbatch=32,lr=0.001,lr_min=1e-6,lr_factor=3,lr_patience=5):\n",
        "        self.model=model\n",
        "        self.model_old=deepcopy(self.get_param(self.model))\n",
        "        self.fisher=None\n",
        "        self.tasks = []\n",
        "\n",
        "        self.nepochs = nepochs\n",
        "        self.sbatch = sbatch\n",
        "        self.lr = lr\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_factor = lr_factor\n",
        "        self.lr_patience = lr_patience\n",
        "        self.lamb = lamb\n",
        "\n",
        "\n",
        "    def add_task(self, src_vocab_size, trg_vocab_size, src_token, trg_token):\n",
        "        if {'src': src_token, 'trg': trg_token} in self.tasks:\n",
        "            return\n",
        "        self.tasks.append({'src': src_token, 'trg': trg_token})\n",
        "        self.model.add_task(src_vocab_size, trg_vocab_size, src_token, trg_token)\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "    def load_task(self, t):\n",
        "        path = '/content/drive/My Drive/Project3/trained_model/ewc_big|{}'.format(\n",
        "                        '|'.join(['%s-%s'%(task['src'], task['trg']) for task in self.tasks[:t]]))\n",
        "        self.model.load_state_dict(torch.load('%s.pt'%path))\n",
        "        self.model.to(device)\n",
        "\n",
        "    def get_param(self, model):\n",
        "        sd = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'word_embedding' in name or 'fc_out' in name or 'position_embedding' in name or 'norm' in name or 'norm1' in name or 'norm2' in name:\n",
        "                continue \n",
        "            sd.append((name, param))\n",
        "        return sd\n",
        "   \n",
        "    def train(self, train_data, valid_data, src_vocab_size, trg_vocab_size, src_token, trg_token):\n",
        "        best_loss = np.inf\n",
        "        lr = self.lr\n",
        "        patience = self.lr_patience\n",
        "        train_iterator = BucketIterator(train_data, batch_size=self.sbatch, device=device, shuffle=True)\n",
        "        # valid_iterator = BucketIterator(valid_data, batch_size=self.sbatch, device=device, shuffle=False)\n",
        "        self.add_task(src_vocab_size, trg_vocab_size, src_token, trg_token)\n",
        "        self.ce=torch.nn.CrossEntropyLoss(ignore_index=self.model.src_pad_idx)\n",
        "        self.optimizer=torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        path = '/content/drive/My Drive/Project3/trained_model/ewc_big|{}'.format(\n",
        "                        '|'.join(['%s-%s'%(task['src'], task['trg']) for task in self.tasks]))\n",
        "        # Loop epochs\n",
        "        for e in range(self.nepochs):\n",
        "            # try:\n",
        "                # Train\n",
        "                clock0=time.time()\n",
        "                train_loss = self.train_epoch(train_iterator,src_token,trg_token)\n",
        "                clock1=time.time()\n",
        "                # valid_loss = self.eval(valid_iterator,src_token,trg_token)\n",
        "                clock2=time.time()\n",
        "                print('| Epoch {:3d}, time={:5.1f}ms/{:5.1f}ms | Train: loss={:.3f} |'.format(\n",
        "                    e+1,1000*(clock1-clock0),\n",
        "                    1000*(clock2-clock1),train_loss),end='')\n",
        "                # # Valid\n",
        "                # print(' Valid: loss={:.3f} |'.format(valid_loss),end='')\n",
        "                \n",
        "                # # Adapt lr\n",
        "                # if valid_loss < best_loss:\n",
        "                #     best_loss = valid_loss\n",
        "                #     torch.save(self.model.state_dict(),'%s.pt' % path)\n",
        "                #     patience = self.lr_patience\n",
        "                #     print(' *', end='')\n",
        "                \n",
        "                # else:\n",
        "                #     patience -= 1\n",
        "                #     if patience <= 0:\n",
        "                #         lr /= self.lr_factor\n",
        "                #         print(' lr={:.1e}'.format(lr), end='')\n",
        "                #         if lr < self.lr_min:\n",
        "                #             print()\n",
        "                            \n",
        "                #         patience = self.lr_patience\n",
        "                #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "                print()\n",
        "            # except KeyboardInterrupt:\n",
        "            #     break\n",
        "\n",
        "        # Restore best\n",
        "        torch.save(self.model.state_dict(),'%s.pt' % path)\n",
        "        # self.model.load_state_dict(torch.load('%s.pt' % path))\n",
        "        # Update old\n",
        "        self.model_old = deepcopy(self.get_param(self.model))\n",
        "        \n",
        "        # Fisher ops\n",
        "        if len(self.tasks) > 1:\n",
        "            fisher_old={}\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                fisher_old[n]=self.fisher[n].clone()\n",
        "        self.fisher=self.fisher_matrix_diag(train_iterator, src_token, trg_token)\n",
        "        if len(self.tasks) > 1:\n",
        "            t = len(self.tasks)-1\n",
        "            # Watch out! We do not want to keep t models (or fisher diagonals) in memory, therefore we have to merge fisher diagonals\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                self.fisher[n]=(self.fisher[n]+fisher_old[n]*t)/(t+1)       # Checked: it is better than the other option\n",
        "                #self.fisher[n]=0.5*(self.fisher[n]+fisher_old[n])\n",
        "\n",
        "        return\n",
        "\n",
        "    def train_epoch(self,iterator,src_token,trg_token):\n",
        "        self.model.train()\n",
        "        total_loss=0\n",
        "        # Loop batches\n",
        "        for batch in iterator:\n",
        "            src = batch.src.permute(1,0).to(device)\n",
        "            trg = batch.trg.permute(1,0).to(device)\n",
        "            output = self.model(src, trg[:, :-1], src_token, trg_token)\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)   \n",
        "            loss=self.criterion(output,trg)\n",
        "            total_loss += loss.item()\n",
        "            # Backward\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return total_loss/len(iterator)\n",
        "\n",
        "    def eval(self,iterator,src_token,trg_token):\n",
        "        total_loss=0\n",
        "        self.model.eval()\n",
        "        # Loop batches\n",
        "        for batch in iterator:\n",
        "            src = batch.src.permute(1,0).to(device)\n",
        "            trg = batch.trg.permute(1,0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = self.model(src, trg[:, :-1], src_token, trg_token)\n",
        "                output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)   \n",
        "            loss=self.criterion(output,trg)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        return total_loss/len(iterator)\n",
        "\n",
        "    def criterion(self,output,targets):\n",
        "        # Regularization for all previous tasks\n",
        "        loss_reg=0\n",
        "        if len(self.tasks) > 1:\n",
        "            for (name,param),(_,param_old) in zip(self.get_param(self.model),self.model_old):   \n",
        "                # print(self.fisher[name].shape, param.shape, param_old.shape)\n",
        "                loss_reg+=torch.sum(self.fisher[name]*(param_old-param).pow(2))/2\n",
        "        return self.ce(output,targets)+self.lamb*loss_reg\n",
        "\n",
        "    def fisher_matrix_diag(self, iterator, src_token, trg_token):\n",
        "        # Init\n",
        "        fisher={}\n",
        "        for n,p in self.get_param(self.model):\n",
        "            fisher[n]=0*p.data\n",
        "        # Compute\n",
        "        self.model.train()\n",
        "        \n",
        "        for batch in iterator:\n",
        "          \n",
        "            # Forward and backward\n",
        "            model.zero_grad()\n",
        "            src = batch.src.permute(1,0).to(device)\n",
        "            trg = batch.trg.permute(1,0).to(device)\n",
        "            output = self.model(src, trg[:, :-1], src_token, trg_token)\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)\n",
        "            loss=self.criterion(output,trg)\n",
        "            loss.backward()\n",
        "            # Get gradients\n",
        "            for n,p in self.get_param(self.model):\n",
        "                if p.grad is not None:\n",
        "                    fisher[n]+=src.shape[0]*p.grad.data.pow(2)\n",
        "        # Mean\n",
        "        with torch.no_grad():\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                fisher[n]=fisher[n]/len(iterator)\n",
        "        return fisher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uye-zTSMTZPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1872b91c-1e03-4a16-b22f-7ec1f8f23927"
      },
      "source": [
        "import random\n",
        "\n",
        "eng = Field(sequential=True, \n",
        "            use_vocab=True,\n",
        "            init_token='<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "viet = Field(sequential=True, \n",
        "            use_vocab=True,\n",
        "            init_token='<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "fields = {'eng': ('src', eng), 'viet': ('trg', viet)}\n",
        "train_eng_viet = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_big_%s.json'%('eng', 'viet', 'train'), format='json', fields=fields)\n",
        "test_eng_viet = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_big_%s.json'%('eng', 'viet', 'tst'), format='json', fields=fields)\n",
        "# valid_data = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_valid.json'%('eng', 'viet'), format='json', fields=fields)\n",
        "\n",
        "\n",
        "eng.build_vocab(train_eng_viet, min_freq=0)\n",
        "\n",
        "viet.build_vocab(train_eng_viet, min_freq=0)\n",
        "\n",
        "eng_vocab_size = len(eng.vocab)\n",
        "viet_vocab_size = len(viet.vocab)\n",
        "print(eng_vocab_size)\n",
        "print(viet_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23926\n",
            "20712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpJ5OEqwLcqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d26ceb-309c-446e-aebd-5d691be6f88c"
      },
      "source": [
        "max_length = max([(len(e.src), len(e.trg)) for e in train_eng_viet.examples])\n",
        "print(max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(241, 236)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkmqCQ_JLEZD"
      },
      "source": [
        "model = Transformer(\n",
        "    src_pad_idx=eng.vocab.stoi['<pad>'],\n",
        "    trg_pad_idx=viet.vocab.stoi['<pad>'],\n",
        "    embed_size=512,\n",
        "    num_layers=1,\n",
        "    forward_expansion=3,\n",
        "    heads=8,\n",
        "    dropout=0.1,\n",
        "    max_length=250,\n",
        ").to(device)\n",
        "\n",
        "ewc = EWC(\n",
        "    model=model,\n",
        "    lamb=4000,\n",
        "    nepochs=50,\n",
        "    sbatch=32,\n",
        "    lr=0.001,\n",
        "    lr_min=1e-6,\n",
        "    lr_factor=3,\n",
        "    lr_patience=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QJf-njWLxjV",
        "outputId": "dc3171b5-96ed-40a2-cee5-276da34a309e"
      },
      "source": [
        "ewc.train(train_eng_viet, None, eng_vocab_size, viet_vocab_size, 'eng', 'viet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=2368.3ms/  0.0ms | Train: loss=0.564 |\n",
            "| Epoch   2, time=2367.4ms/  0.0ms | Train: loss=0.539 |\n",
            "| Epoch   3, time=2374.9ms/  0.0ms | Train: loss=0.530 |\n",
            "| Epoch   4, time=2375.9ms/  0.0ms | Train: loss=0.518 |\n",
            "| Epoch   5, time=2377.0ms/  0.0ms | Train: loss=0.510 |\n",
            "| Epoch   6, time=2380.2ms/  0.0ms | Train: loss=0.501 |\n",
            "| Epoch   7, time=2376.3ms/  0.0ms | Train: loss=0.494 |\n",
            "| Epoch   8, time=2391.1ms/  0.0ms | Train: loss=0.481 |\n",
            "| Epoch   9, time=2380.0ms/  0.0ms | Train: loss=0.474 |\n",
            "| Epoch  10, time=2364.2ms/  0.0ms | Train: loss=0.469 |\n",
            "| Epoch  11, time=2371.7ms/  0.0ms | Train: loss=0.458 |\n",
            "| Epoch  12, time=2376.1ms/  0.0ms | Train: loss=0.454 |\n",
            "| Epoch  13, time=2373.0ms/  0.0ms | Train: loss=0.447 |\n",
            "| Epoch  14, time=2373.6ms/  0.0ms | Train: loss=0.436 |\n",
            "| Epoch  15, time=2368.3ms/  0.0ms | Train: loss=0.433 |\n",
            "| Epoch  16, time=2368.3ms/  0.0ms | Train: loss=0.424 |\n",
            "| Epoch  17, time=2369.7ms/  0.0ms | Train: loss=0.415 |\n",
            "| Epoch  18, time=2373.8ms/  0.0ms | Train: loss=0.415 |\n",
            "| Epoch  19, time=2369.8ms/  0.0ms | Train: loss=0.407 |\n",
            "| Epoch  20, time=2375.0ms/  0.0ms | Train: loss=0.402 |\n",
            "| Epoch  21, time=2375.0ms/  0.0ms | Train: loss=0.399 |\n",
            "| Epoch  22, time=2361.1ms/  0.0ms | Train: loss=0.394 |\n",
            "| Epoch  23, time=2375.8ms/  0.0ms | Train: loss=0.387 |\n",
            "| Epoch  24, time=2370.2ms/  0.0ms | Train: loss=0.381 |\n",
            "| Epoch  25, time=2376.6ms/  0.0ms | Train: loss=0.380 |\n",
            "| Epoch  26, time=2357.4ms/  0.0ms | Train: loss=0.373 |\n",
            "| Epoch  27, time=2368.7ms/  0.0ms | Train: loss=0.369 |\n",
            "| Epoch  28, time=2377.0ms/  0.0ms | Train: loss=0.365 |\n",
            "| Epoch  29, time=2374.7ms/  0.0ms | Train: loss=0.359 |\n",
            "| Epoch  30, time=2376.3ms/  0.0ms | Train: loss=0.356 |\n",
            "| Epoch  31, time=2381.1ms/  0.0ms | Train: loss=0.349 |\n",
            "| Epoch  32, time=2378.3ms/  0.0ms | Train: loss=0.348 |\n",
            "| Epoch  33, time=2363.3ms/  0.0ms | Train: loss=0.342 |\n",
            "| Epoch  34, time=2377.1ms/  0.0ms | Train: loss=0.335 |\n",
            "| Epoch  35, time=2359.9ms/  0.0ms | Train: loss=0.335 |\n",
            "| Epoch  36, time=2379.1ms/  0.0ms | Train: loss=0.333 |\n",
            "| Epoch  37, time=2380.1ms/  0.0ms | Train: loss=0.327 |\n",
            "| Epoch  38, time=2383.1ms/  0.0ms | Train: loss=0.327 |\n",
            "| Epoch  39, time=2366.8ms/  0.0ms | Train: loss=0.321 |\n",
            "| Epoch  40, time=2378.9ms/  0.0ms | Train: loss=0.316 |\n",
            "| Epoch  41, time=2374.0ms/  0.0ms | Train: loss=0.314 |\n",
            "| Epoch  42, time=2381.4ms/  0.0ms | Train: loss=0.312 |\n",
            "| Epoch  43, time=2368.6ms/  0.0ms | Train: loss=0.309 |\n",
            "| Epoch  44, time=2361.0ms/  0.0ms | Train: loss=0.305 |\n",
            "| Epoch  45, time=2377.4ms/  0.0ms | Train: loss=0.303 |\n",
            "| Epoch  46, time=2368.4ms/  0.0ms | Train: loss=0.299 |\n",
            "| Epoch  47, time=2375.7ms/  0.0ms | Train: loss=0.294 |\n",
            "| Epoch  48, time=2367.7ms/  0.0ms | Train: loss=0.296 |\n",
            "| Epoch  49, time=2363.1ms/  0.0ms | Train: loss=0.291 |\n",
            "| Epoch  50, time=2379.6ms/  0.0ms | Train: loss=0.289 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd--v15zhyj1",
        "outputId": "6fd48d11-365f-417a-c99e-8e060644c960"
      },
      "source": [
        "import random\n",
        "\n",
        "def text_prepocess(text):\n",
        "    return re.split('\\W+', ' '.join(text))\n",
        "\n",
        "eng = Field(sequential=True, \n",
        "            use_vocab=True,\n",
        "            preprocessing = text_prepocess,\n",
        "            init_token='<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "viet = Field(sequential=True, \n",
        "            use_vocab=True,\n",
        "            preprocessing = text_prepocess,\n",
        "            init_token='<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "fields = {'eng': ('trg', eng), 'viet': ('src', viet)}\n",
        "train_viet_eng = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_big_%s.json'%('eng', 'viet', 'train'), format='json', fields=fields)\n",
        "test_viet_eng = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_big_%s.json'%('eng', 'viet', 'tst'), format='json', fields=fields)\n",
        "# valid_data = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_valid.json'%('eng', 'viet'), format='json', fields=fields)\n",
        "\n",
        "\n",
        "eng.build_vocab(train_viet_eng, min_freq=0)\n",
        "\n",
        "viet.build_vocab(train_viet_eng, min_freq=0)\n",
        "\n",
        "eng_vocab_size = len(eng.vocab)\n",
        "viet_vocab_size = len(viet.vocab)\n",
        "print(eng_vocab_size)\n",
        "print(viet_vocab_size)\n",
        "print(len(train_viet_eng.examples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23926\n",
            "20712\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek_Zie90jStk",
        "outputId": "4207b5eb-39f0-49af-dd66-589e7d7c4bfc"
      },
      "source": [
        "ewc.train(train_viet_eng, None, viet_vocab_size, eng_vocab_size, 'viet', 'eng')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=2953.2ms/  0.0ms | Train: loss=6.448 |\n",
            "| Epoch   2, time=2964.2ms/  0.0ms | Train: loss=4.915 |\n",
            "| Epoch   3, time=2960.2ms/  0.0ms | Train: loss=4.041 |\n",
            "| Epoch   4, time=2987.6ms/  0.0ms | Train: loss=3.480 |\n",
            "| Epoch   5, time=2987.8ms/  0.0ms | Train: loss=3.181 |\n",
            "| Epoch   6, time=2990.3ms/  0.0ms | Train: loss=2.970 |\n",
            "| Epoch   7, time=2979.9ms/  0.0ms | Train: loss=2.800 |\n",
            "| Epoch   8, time=2994.0ms/  0.0ms | Train: loss=2.661 |\n",
            "| Epoch   9, time=2990.1ms/  0.0ms | Train: loss=2.543 |\n",
            "| Epoch  10, time=2974.0ms/  0.0ms | Train: loss=2.436 |\n",
            "| Epoch  11, time=2976.0ms/  0.0ms | Train: loss=2.344 |\n",
            "| Epoch  12, time=2984.4ms/  0.0ms | Train: loss=2.253 |\n",
            "| Epoch  13, time=2977.3ms/  0.0ms | Train: loss=2.181 |\n",
            "| Epoch  14, time=2978.2ms/  0.0ms | Train: loss=2.105 |\n",
            "| Epoch  15, time=2978.6ms/  0.0ms | Train: loss=2.043 |\n",
            "| Epoch  16, time=2981.9ms/  0.0ms | Train: loss=1.975 |\n",
            "| Epoch  17, time=2976.5ms/  0.0ms | Train: loss=1.917 |\n",
            "| Epoch  18, time=2983.7ms/  0.0ms | Train: loss=1.864 |\n",
            "| Epoch  19, time=2980.8ms/  0.0ms | Train: loss=1.808 |\n",
            "| Epoch  20, time=2976.0ms/  0.0ms | Train: loss=1.758 |\n",
            "| Epoch  21, time=2986.5ms/  0.0ms | Train: loss=1.710 |\n",
            "| Epoch  22, time=2970.0ms/  0.0ms | Train: loss=1.662 |\n",
            "| Epoch  23, time=2976.3ms/  0.0ms | Train: loss=1.620 |\n",
            "| Epoch  24, time=2979.2ms/  0.0ms | Train: loss=1.577 |\n",
            "| Epoch  25, time=2982.1ms/  0.0ms | Train: loss=1.542 |\n",
            "| Epoch  26, time=2970.0ms/  0.0ms | Train: loss=1.502 |\n",
            "| Epoch  27, time=2982.2ms/  0.0ms | Train: loss=1.462 |\n",
            "| Epoch  28, time=2977.2ms/  0.0ms | Train: loss=1.429 |\n",
            "| Epoch  29, time=2981.3ms/  0.0ms | Train: loss=1.396 |\n",
            "| Epoch  30, time=2986.0ms/  0.0ms | Train: loss=1.364 |\n",
            "| Epoch  31, time=2993.3ms/  0.0ms | Train: loss=1.330 |\n",
            "| Epoch  32, time=2979.6ms/  0.0ms | Train: loss=1.301 |\n",
            "| Epoch  33, time=2972.9ms/  0.0ms | Train: loss=1.276 |\n",
            "| Epoch  34, time=2979.8ms/  0.0ms | Train: loss=1.244 |\n",
            "| Epoch  35, time=2973.1ms/  0.0ms | Train: loss=1.220 |\n",
            "| Epoch  36, time=2983.4ms/  0.0ms | Train: loss=1.192 |\n",
            "| Epoch  37, time=2989.6ms/  0.0ms | Train: loss=1.163 |\n",
            "| Epoch  38, time=2991.5ms/  0.0ms | Train: loss=1.139 |\n",
            "| Epoch  39, time=2978.1ms/  0.0ms | Train: loss=1.118 |\n",
            "| Epoch  40, time=2995.1ms/  0.0ms | Train: loss=1.096 |\n",
            "| Epoch  41, time=2983.9ms/  0.0ms | Train: loss=1.072 |\n",
            "| Epoch  42, time=3002.0ms/  0.0ms | Train: loss=1.049 |\n",
            "| Epoch  43, time=2970.3ms/  0.0ms | Train: loss=1.031 |\n",
            "| Epoch  44, time=2982.4ms/  0.0ms | Train: loss=1.008 |\n",
            "| Epoch  45, time=2986.9ms/  0.0ms | Train: loss=0.991 |\n",
            "| Epoch  46, time=2986.3ms/  0.0ms | Train: loss=0.972 |\n",
            "| Epoch  47, time=2975.8ms/  0.0ms | Train: loss=0.952 |\n",
            "| Epoch  48, time=2977.4ms/  0.0ms | Train: loss=0.936 |\n",
            "| Epoch  49, time=2958.9ms/  0.0ms | Train: loss=0.919 |\n",
            "| Epoch  50, time=2969.9ms/  0.0ms | Train: loss=0.901 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUA-NBSMjZWZ"
      },
      "source": [
        "def eval_sen(model, src_sen, trg_sen, src, trg, src_lang, trg_lang, max_length=250):\n",
        "\n",
        "    model.eval()\n",
        "    src_tensor = src_sen.lower().split()\n",
        "    src_tensor.insert(0, src.init_token)\n",
        "    src_tensor.append(src.eos_token)\n",
        "    src_tensor = [src.vocab.stoi[i] for i in src_tensor]\n",
        "    src_tensor = torch.LongTensor(src_tensor).unsqueeze(0).to(device)\n",
        "    outputs = [trg.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(max_length):\n",
        "        translate_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, translate_tensor, src_lang, trg_lang)\n",
        "        \n",
        "        best_guess = output.argmax(2)[:,-1].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == trg.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    outputs = outputs[1:-1]\n",
        "    translated_sentence = [trg.vocab.itos[idx] for idx in outputs]\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split(' ')], translated_sentence, weights=(0.5, 0.5))\n",
        "    # print(translated_sentence)\n",
        "    # print(trg_sen.split(' '))\n",
        "    return ' '.join(translated_sentence), bleu_score\n",
        "\n",
        "def eval(model, data, src_lang, trg_lang, src_token, trg_token):\n",
        "    total_bleu = 0\n",
        "    for example in data.examples:\n",
        "        src_sen = ' '.join(example.src)\n",
        "        trg_sen = ' '.join(example.trg)\n",
        "        translated_sentence, bleu_score = eval_sen(model, src_sen, trg_sen, src_lang, trg_lang, src_token, trg_token)\n",
        "        total_bleu += bleu_score\n",
        "    return total_bleu / len(data.examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgS42HAEqKz0",
        "outputId": "64e6a212-5df1-486d-abd7-9655c818aae5"
      },
      "source": [
        "# before learn task 2\r\n",
        "print('task 1: eng-viet, bleu score:', eval(model, test_eng_viet, eng, viet, 'eng', 'viet'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1: eng-viet, bleu score: 0.31722327460644056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdwVA6YsPbSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320a6943-9183-4d3b-d1e0-bf1c1a82b032"
      },
      "source": [
        "# after learn task 2\n",
        "print('task 2: viet-eng, bleu score:', eval(model, test_viet_eng, viet, eng, 'viet', 'eng'))\n",
        "print('task 1: eng-viet, bleu score:', eval(model, test_eng_viet, eng, viet, 'eng', 'viet'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 2: viet-eng, bleu score: 0.29126687596523426\n",
            "task 1: eng-viet, bleu score: 0.28713554416652876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwmlDb9as1SW",
        "outputId": "f8195da9-bcae-4d86-84ea-070ed470ce5d"
      },
      "source": [
        "print('task 1: eng-viet, bleu score:', eval(model, train_eng_viet, eng, viet, 'eng', 'viet'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1: eng-viet, bleu score: 0.9543622793442226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoRfbPMsk6of",
        "outputId": "05ce7139-8b15-4ce0-f950-84f09f4a509d"
      },
      "source": [
        "print('task 2: viet-eng, bleu score:', eval(model, train_viet_eng, viet, eng, 'viet', 'eng'))\r\n",
        "print('task 1: eng-viet, bleu score:', eval(model, train_eng_viet, eng, viet, 'eng', 'viet'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pOGHsNedv2G"
      },
      "source": [
        "ewc.add_task(eng_vocab_size, viet_vocab_size, 'eng', 'viet')# task 1\n",
        "ewc.add_task(viet_vocab_size, eng_vocab_size, 'viet', 'eng')# task 2\n",
        "ewc.load_task(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVxL3fdhzPjS",
        "outputId": "b311405d-052c-41c9-8abb-28760b0f1ebc"
      },
      "source": [
        "# np.random.seed(10)\n",
        "\n",
        "for example in np.random.choice(test_eng_viet.examples, 10, replace=False):\n",
        "    src_sen = ' '.join(example.src)\n",
        "    trg_sen = ' '.join(example.trg)\n",
        "    translated_sentence, bleu_score = eval_sen(model, src_sen, trg_sen, eng, viet, 'eng', 'viet')\n",
        "    print()\n",
        "    print('> source sentence:', src_sen)\n",
        "    print('< translated sentence:', translated_sentence)\n",
        "    print('= correct sentence:', trg_sen)\n",
        "    print('bleu score:',bleu_score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: in addition if you give it enough information to realize that you are a shoe enthusiast the hottest\n",
            "< translated sentence: ngoài_ra nó nếu bạn hiểu rất đủ lực_lượng để nhận ra cách một cuộc tấn_công sản_phẩm\n",
            "= correct sentence: ngoài_ra nếu cho nó đủ dữ_kiện để nhận ra rằng bạn là một đầu giày những thông_tin nóng_sốt nhất về thế_giới giày_dép sẽ được hiển_thị đầy_đủ có_vẻ như đây là cung_cách phục_vụ thông_minh dù khách_hàng có tìm đến sản_phẩm của adidas vì chức_năng hay phong_cách\n",
            "bleu score: 0.04712492027964564\n",
            "\n",
            "> source sentence: the bun rieu cua is a vermicelli soup with a tomato based broth made by slowly simmering pork or chicken bones topped with fried tofu prawns crab meat bean sprouts and fresh vietnamese herbs like perilla and cilantro\n",
            "< translated sentence: món ăn này có tên bún riêu cua là một bát nước_dùng cốp cà được chọn bằng cách hành tỏi lá chúc sả trứng và bánh_quy với giá dầu này thông_qua quy_trình này nhằm phục_vụ món ăn này và rau mùi việt_nam\n",
            "= correct sentence: bún riêu cua là một món bún với nước_dùng cà_chua hầm xương gà hoặc lợn với đậu_phụ chiên tôm thịt cua giá đỗ và các loại rau_thơm ở việt_nam như tía_tô và ngò\n",
            "bleu score: 0.16695677422593644\n",
            "\n",
            "> source sentence: the repatriation flight was operated with the cooperation of vietnamese and thai authorities and national flag carrier vietnam airlines\n",
            "< translated sentence: chuyến bay hồi_hương từ chuyến bay đến tp hcm và sự hợp_tác của hãng hàng không quốc_gia việt_nam\n",
            "= correct sentence: chuyến bay hồi_hương này được vận_hành với sự hợp_tác của chính_quyền việt_nam và thái_lan và hãng hàng không quốc_gia vietnam_airlines\n",
            "bleu score: 0.4688392767968134\n",
            "\n",
            "> source sentence: asbestosis affected about 157 000 people and resulted in 3 600 deaths in 2015\n",
            "< translated sentence: khoảng 157 người bị ảnh_hưởng đến 500 000 người chết năm 2015 trên khắp thế_giới\n",
            "= correct sentence: bệnh bụi phổi amiăng ảnh_hưởng đến khoảng 157 000 người và khiến 3 600 người tử_vong trong năm 2015\n",
            "bleu score: 0.31712446033785674\n",
            "\n",
            "> source sentence: plagiarizing can range from reusing others music without authorization or giving proper credit and copying ideas from others mvs to imitating choreography and style\n",
            "< translated sentence: tiếng trung có_thể từ lâu rồi hoặc những người không phát_hành nào đó là biểu_tượng cho lệnh cấm một_cách hiệu_quả và những gì chúng_ta sẽ làm cạn_kiệt\n",
            "= correct sentence: việc đạo nhạc có_thể trải dài từ việc tái sử_dụng âm_nhạc của người khác mà không xin phép bản_quyền hoặc ghi nguồn phù_hợp và sao_chép ý_tưởng từ mv của người khác cho đến bắt_chước vũ_đạo và phong_cách\n",
            "bleu score: 0.3515744692560821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: it s still hard to be hopeful when you re not seeing anyone doing well at the moment she said\n",
            "< translated sentence: lyons nói vẫn là mình hy_vọng rằng bạn vẫn chưa biết rằng mình đấy khá rồi cô yêu mình cô đấy\n",
            "= correct sentence: còn rất khó để hy_vọng khi bạn không nhìn thấy bất_cứ ai đang ổn_định tại thời_điểm hiện_tại cô cho biết\n",
            "bleu score: 0.4364357804719847\n",
            "\n",
            "> source sentence: specifically those who develop lung disease within 14 days of returning from wuhan are under surveillance\n",
            "< translated sentence: cụ_thể cũng đang nỗ_lực những trường_hợp nhiễm_bệnh trong 14 ngày qua tất_cả đều trở về từ vũ_hán\n",
            "= correct sentence: cụ_thể những người phát bệnh phổi trong vòng 14 ngày kể từ khi trở về từ vũ_hán nằm trong diện theo_dõi\n",
            "bleu score: 0.2875278132838648\n",
            "\n",
            "> source sentence: two other babies delivered in da nang and quang nam province in central vietnam weighed 6 5 kilos at birth\n",
            "< translated sentence: hai trẻ_em khác ở tỉnh quảng_nam và đàn_ông đã hạ_sinh một 5 kg cam_kết hỗ_trợ 6 5 kg\n",
            "= correct sentence: hai em bé khác sinh tại đà_nẵng và tỉnh quảng_nam ở miền trung việt_nam nặng 6 5 kg lúc mới sinh\n",
            "bleu score: 0.2514425400225296\n",
            "\n",
            "> source sentence: that makes forests from the amazon to siberia vast natural stores of greenhouse gases\n",
            "< translated sentence: rừng phát ra từ rừng amazon đến nay không chiếm phần_lớn các cửa_hàng tiện_lợi khí nhà_kính\n",
            "= correct sentence: điều đó làm cho các khu rừng từ amazon đến siberia trở_thành các kho khí nhà_kính tự_nhiên khổng_lồ\n",
            "bleu score: 0.21314325154497693\n",
            "\n",
            "> source sentence: a moth that flies full speed into such a trap does n t stand much of a chance of freeing itself\n",
            "< translated sentence: một ruồi cắn đó sẽ cố đẩy nhanh để vào tay họ không bản_thân\n",
            "= correct sentence: một chú bướm bay hết tốc_lực vào một cái bẫy như_vậy sẽ hầu_như không có cơ_hội nào để thoát ra được\n",
            "bleu score: 0.3624713984635016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZEiJeTZyBN0",
        "outputId": "a1ce75c4-5f05-486a-a347-99706c465a39"
      },
      "source": [
        "\n",
        "for example in np.random.choice(test_viet_eng.examples, 10, replace=False):\n",
        "    src_sen = ' '.join(example.src)\n",
        "    trg_sen = ' '.join(example.trg)\n",
        "    translated_sentence, bleu_score = eval_sen(model, src_sen, trg_sen, viet, eng, 'viet', 'eng')\n",
        "    print()\n",
        "    print('> source sentence:', src_sen)\n",
        "    print('< translated sentence:', translated_sentence)\n",
        "    print('= correct sentence:', trg_sen)\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: các nhà xuất_khẩu chất_thải nhựa hàng_đầu được phân_tích trong báo_cáo bao_gồm hoa_kỳ anh đức và nhật_bản\n",
            "< translated sentence: leading plastic waste exporters are the top of the u s report including japan and germany japan\n",
            "= correct sentence: the top exporters of plastic waste analyzed for the report included the united states britain germany and japan\n",
            "bleu score: 0.2681514527528423\n",
            "\n",
            "> source sentence: nó đã được báo_cáo trên the_guardian rằng các siêu_thị của anh và các nhà cung_cấp của họ đã phát_triển một kế_hoạch để đảm_bảo cung_cấp nhất_quán một loạt các hàng_hóa cơ_bản nếu người tiêu_dùng mua hoảng_loạn\n",
            "< translated sentence: it has been reported that the uk s population and buy a large number of customers developing their plans to ensure a large number of goods companies meet the consumption stimulation programs have buy supply chains buy a consumers\n",
            "= correct sentence: in early march the guardian reported that british supermarkets and their suppliers had developed a plan to ensure a consistent supply of a range of basic goods if there was panic buying by consumers\n",
            "bleu score: 0.17425375883933072\n",
            "\n",
            "> source sentence: lính cứu_hỏa tiếp_xúc với khói thường_xuyên hơn và ở mức_độ nguy_hiểm hơn rất nhiều\n",
            "< translated sentence: the fire is more than 10 percent and more dangerous than others are on average\n",
            "= correct sentence: firefighters get hit with smoke much more often and at much higher levels\n",
            "bleu score: 0.36514837167011077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: vào thứ_bảy trung_tâm kiểm_soát dịch_bệnh ở tỉnh bình_thuận đã cách_ly ba người đàn_ông và một người phụ_nữ trong một cơ_sở tập_trung tại một trường trung_học địa_phương\n",
            "< translated sentence: in saturday the center for disease control and prevention binh thuan province a man and a woman in a one of a high school year old woman in a local islamic school\n",
            "= correct sentence: on saturday the center for disease control in binh thuan province quarantined three men and a woman in a centralized facility at a local high school\n",
            "bleu score: 0.4989909172358461\n",
            "\n",
            "> source sentence: trong bài phát_biểu của mình ellison cho rằng cuộc biểu_tình là kịch_tính và gần thiết nói rằng floyd đáng_lẽ nên ở đây và giờ thì anh ấy không_thể\n",
            "< translated sentence: in his statement said that it was that the protest the pandemic and was found that he should be adopted in and his heart and his position\n",
            "= correct sentence: during his announcement ellison called the protests dramatic and necessary saying floyd should be here and he is not\n",
            "bleu score: 0.09985744825254632\n",
            "\n",
            "> source sentence: điều này có_nghĩa_là từ việc sản_xuất và phân_phối đến doanh việc bán hàng và phản_hồi của khách_hàng của công_ty đã tạo ra những dãy núi dữ_liệu khổng_lồ\n",
            "< translated sentence: this means that creates and the distribution of sale of the sale of customers s customer creates customers now\n",
            "= correct sentence: this means that the company s production and distribution to sales and customer feedback have generated huge mountains of data\n",
            "bleu score: 0.2176534699026074\n",
            "\n",
            "> source sentence: vào ngày 17 tháng 3 bộ_trưởng tài_chính grant_robertson đã công_bố là gói kinh_doanh covid 19 trị_giá 12 1 tỷ đô_la new_zealand bao_gồm 8 7 tỷ đô_la new_zealand cho các doanh_nghiệp và công_việc 2 8 tỷ đô_la new_zealand cho hỗ_trợ thu_nhập 500 triệu đô_la new_zealand cho sức_khỏe và gói hỗ_trợ 600 triệu đô_la new_zealand cho ngành_hàng không và để hỗ_trợ chuỗi cung_ứng điều này không bao_gồm bất_kỳ hỗ_trợ nào cho air new_zealand tàu sân_bay quốc_gia\n",
            "< translated sentence: on 17 march the new zealand government announced that it was ready to support new zealand 1 500 new zealand to zealand and new zealand for zealand to zealand and administration starting thursday evening of the new zealand to support for the new zealand 53 new zealand and administration including assistance for the ship new zealand government s co new zealand and 3 million for zealand and with new zealand to stand its zealand on its marine plastic products\n",
            "= correct sentence: finance minister grant robertson announced a 12 1 billion covid 19 business package that included 8 7 billion for businesses and jobs 2 8 billion for income support 500 million for health and 600 million for the aviation sector and to support supply chains this did not include any support for air new zealand\n",
            "bleu score: 0.12085382706823943\n",
            "\n",
            "> source sentence: vào ngày 11 tháng 3 bộ_trưởng quốc_phòng nga ông serge_shoygu đã hủy bỏ hội_nghị an_ninh quốc_tế moskva đã được lên kế_hoạch vào ngày 22 tháng 7\n",
            "< translated sentence: on 11 march russia announcement he had visited the national defense and was announced on the moscow announced on 7 million 22 january\n",
            "= correct sentence: on 11 march russian defence minister sergey shoygu cancelled the moscow international security conference which had been scheduled for 22 to 23 april\n",
            "bleu score: 0.2037203384166269\n",
            "\n",
            "> source sentence: nhưng sự tập_trung của ông ấy chủ_yếu vào cuộc_sống của những chú chó mà ông ấy cứu thoát khỏi cái chết hơn là kế sinh_nhai của ông ấy tức_là cứu chúng thoát khỏi những người thích ăn thịt chó\n",
            "< translated sentence: but he s focus on life of the dogs could not bring him to save him\n",
            "= correct sentence: but his focus is less on his own livelihood than the lives of dogs he rescues from the jaws of death i e the jaws of dog meat lovers\n",
            "bleu score: 0.2935112568571006\n",
            "\n",
            "> source sentence: trong các trường_hợp đặc_biệt những người nhập_cảnh vào việt_nam vì các mục_đích ngoại_giao hoặc những lý_do đặc_biệt khác sẽ phải hoàn_thành thủ_tục khai tình_trạng sức_khỏe và phải cách_ly trong vòng 14 ngày ông thủ_tướng cho biết\n",
            "< translated sentence: in particular particularly for special entry for vietnamese workers are especially those living or other reason for the prime minister nguyen thi reasons and the health ministry of health for 14 days\n",
            "= correct sentence: in exclusive cases people who had to enter vietnam for state affairs or other special reasons have to complete health declaration procedures and then stay in quarantine for 14 days he ordered\n",
            "bleu score: 0.18239004641772003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TnXxK4ceZ-1",
        "outputId": "f411c544-9196-4ed1-e754-ff6095f8bf52"
      },
      "source": [
        "for example in np.random.choice(train_eng_fra.examples, 10, replace=False):\n",
        "    src_sen = ' '.join(example.src)\n",
        "    trg_sen = ' '.join(example.trg)\n",
        "    translated_sentence, bleu_score = eval_sen(model, src_sen, trg_sen, eng1, fra, 'eng1', 'fra')\n",
        "    print()\n",
        "    print('> source sentence:', src_sen)\n",
        "    print('< translated sentence:', translated_sentence)\n",
        "    print('= correct sentence:', trg_sen)\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: here catch \n",
            "< translated sentence: ici \n",
            "= correct sentence: ici attrape \n",
            "bleu score: 0.6065306597126334\n",
            "\n",
            "> source sentence: don t do that \n",
            "< translated sentence: ne le pas ça \n",
            "= correct sentence: ne fais pas ça \n",
            "bleu score: 0.6324555320336759\n",
            "\n",
            "> source sentence: he dislikes me \n",
            "< translated sentence: il ne me ne me pas pas \n",
            "= correct sentence: il ne m aime pas \n",
            "bleu score: 0.3779644730092272\n",
            "\n",
            "> source sentence: you re foolish \n",
            "< translated sentence: tu es triste \n",
            "= correct sentence: tu es imprudente \n",
            "bleu score: 0.49999999999999994\n",
            "\n",
            "> source sentence: tom s scared \n",
            "< translated sentence: tom est en train de grandiront \n",
            "= correct sentence: tom est apeuré \n",
            "bleu score: 0.26726124191242434\n",
            "\n",
            "> source sentence: i like talking \n",
            "< translated sentence: j aime les maison \n",
            "= correct sentence: j aime causer \n",
            "bleu score: 0.3872983346207417\n",
            "\n",
            "> source sentence: are we leaving \n",
            "< translated sentence: sommes nous \n",
            "= correct sentence: y allons nous \n",
            "bleu score: 0.41368954504257255\n",
            "\n",
            "> source sentence: come inside \n",
            "< translated sentence: viens \n",
            "= correct sentence: entre \n",
            "bleu score: 0.7071067811865476\n",
            "\n",
            "> source sentence: stop screaming \n",
            "< translated sentence: arrête de hurler \n",
            "= correct sentence: arrête de hurler \n",
            "bleu score: 1.0\n",
            "\n",
            "> source sentence: tom shrugged \n",
            "< translated sentence: tom a fait \n",
            "= correct sentence: tom haussa les épaules \n",
            "bleu score: 0.5506953149031838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hVhvMdO77c8",
        "outputId": "174614f3-ca21-43c5-8e77-2c15cea87ba1"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "eng = Field(sequential=True, \r\n",
        "            use_vocab=True,\r\n",
        "            init_token='<sos>',\r\n",
        "            eos_token = '<eos>',\r\n",
        "            lower = True)\r\n",
        "\r\n",
        "viet = Field(sequential=True, \r\n",
        "            use_vocab=True,\r\n",
        "            init_token='<sos>',\r\n",
        "            eos_token = '<eos>',\r\n",
        "            lower = True)\r\n",
        "\r\n",
        "fields = {'eng': ('src', eng), 'viet': ('trg', viet)}\r\n",
        "train_eng_viet = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_train.json'%('ENG', 'VIET'), format='json', fields=fields)\r\n",
        "test_eng_viet = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_test.json'%('ENG', 'VIET'), format='json', fields=fields)\r\n",
        "# valid_data = TabularDataset('/content/drive/My Drive/Project3/data/%s-%s_valid.json'%('eng', 'viet'), format='json', fields=fields)\r\n",
        "\r\n",
        "\r\n",
        "eng.build_vocab(train_eng_viet, min_freq=0)\r\n",
        "\r\n",
        "viet.build_vocab(train_eng_viet, min_freq=0)\r\n",
        "\r\n",
        "eng_vocab_size = len(eng.vocab)\r\n",
        "viet_vocab_size = len(viet.vocab)\r\n",
        "print(eng_vocab_size)\r\n",
        "print(viet_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3672\n",
            "2354\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}