{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilingual_NMT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibaso/Project3_quan.nt173312/blob/main/Multilingual_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlI_GUf4lBcT",
        "outputId": "d123cddb-76d9-4c5c-9974-156bf1771c74"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=879ecceba60d26103780a4feaec89f07ea6ae95ebec2dc4dda42b3a82c0897f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI-_YCKtOWXY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "def split_data(path):\r\n",
        "    with open('/content/drive/My Drive/Project3/data/%s.txt'%path, 'r', encoding='utf-8') as f:\r\n",
        "        data = list(f.read().splitlines())\r\n",
        "        train, test = train_test_split(data, train_size=0.85, shuffle=True, random_state=1)\r\n",
        "        train, valid = train_test_split(train, train_size=0.85, shuffle=True, random_state=1)\r\n",
        "        with open('/content/drive/My Drive/Project3/data/%s_train.txt'%path, 'w', encoding='utf-8') as f:\r\n",
        "            f.write('\\n'.join(train))\r\n",
        "        with open('/content/drive/My Drive/Project3/data/%s_valid.txt'%path, 'w', encoding='utf-8') as f:\r\n",
        "            f.write('\\n'.join(valid))\r\n",
        "        with open('/content/drive/My Drive/Project3/data/%s_test.txt'%path, 'w', encoding='utf-8') as f:\r\n",
        "            f.write('\\n'.join(test))\r\n",
        "\r\n",
        "split_data('eng-deu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqY_uGXeTZPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba3c322-fe40-4ff8-8330-d0b838181fe7"
      },
      "source": [
        "import torchtext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset, Example\n",
        "from torchsummary import summary\n",
        "import math\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import os\n",
        "import time\n",
        "import nltk\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# device='cpu'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM1iP2RRlWrm"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\r\n",
        "# MultilingualBert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9nYhH3wluWY"
      },
      "source": [
        "seq1 = \"hello world\"\r\n",
        "seq2 = \"xin chào thế giới\"\r\n",
        "encoded_input = tokenizer([seq1, seq2], return_tensors='pt', padding=True)\r\n",
        "trg = encoded_input['input_ids']\r\n",
        "print(trg)\r\n",
        "print(encoded_input['attention_mask'])\r\n",
        "N, trg_len = trg.shape\r\n",
        "trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\r\n",
        "    trg_len, trg_len\r\n",
        ")\r\n",
        "print(trg_mask)\r\n",
        "output = MultilingualBert(input_ids=trg, attention_mask=encoded_input['attention_mask'])\r\n",
        "encoded_input = tokenizer(seq2)\r\n",
        "print(encoded_input)\r\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))\r\n",
        "print(output.last_hidden_state.shape)\r\n",
        "print(output.pooler_output.shape)\r\n",
        "# print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKR-0-q6UAAT",
        "outputId": "faf93658-9c65-4929-8f70-1f2e80b3d2a8"
      },
      "source": [
        "encoded_input = tokenizer(eng_train[1])\r\n",
        "print(encoded_input)\r\n",
        "print(eng_train[1].split(' '))\r\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]).split(' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 12277, 13028, 12796, 31862, 16683, 13172, 13028, 22469, 11783, 33264, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "['if', 'you', 'worry', 'too', 'much', 'you', 'll', 'go', 'bald', '']\n",
            "['[CLS]', 'if', 'you', 'worry', 'too', 'much', 'you', 'll', 'go', 'bald', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prE5hFenDUP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a52e90-353d-4ae4-adab-a4ea7b64f41b"
      },
      "source": [
        "energy = torch.ones(2,3,3)\r\n",
        "mask = torch.tril(torch.ones((3, 3))).expand(\r\n",
        "    3, 3\r\n",
        ")\r\n",
        "print(mask)\r\n",
        "energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\r\n",
        "print(energy)\r\n",
        "mask = torch.ones(3)\r\n",
        "print(mask)\r\n",
        "energy = torch.ones(3,3)\r\n",
        "energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\r\n",
        "print(energy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[[ 1.0000e+00, -1.0000e+20, -1.0000e+20],\n",
            "         [ 1.0000e+00,  1.0000e+00, -1.0000e+20],\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[ 1.0000e+00, -1.0000e+20, -1.0000e+20],\n",
            "         [ 1.0000e+00,  1.0000e+00, -1.0000e+20],\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00]]])\n",
            "tensor([1., 1., 1.])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqU6C36lzVG"
      },
      "source": [
        "\r\n",
        "class SelfAttention(nn.Module):\r\n",
        "    def __init__(self, embed_size, heads):\r\n",
        "        super(SelfAttention, self).__init__()\r\n",
        "        self.embed_size = embed_size\r\n",
        "        self.heads = heads\r\n",
        "        self.head_dim = embed_size // heads\r\n",
        "\r\n",
        "        assert (\r\n",
        "            self.head_dim * heads == embed_size\r\n",
        "        ), \"Embedding size needs to be divisible by heads\"\r\n",
        "\r\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "        self.att_fc = nn.Linear(heads * self.head_dim, embed_size)\r\n",
        "\r\n",
        "    def forward(self, values, keys, query, mask):\r\n",
        "        # Get number of training examples\r\n",
        "        N = query.shape[0]\r\n",
        "\r\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\r\n",
        "\r\n",
        "        # Split the embedding into self.heads different pieces\r\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\r\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\r\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\r\n",
        "\r\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\r\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\r\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\r\n",
        "\r\n",
        "        # Einsum does matrix mult. for query*keys for each training example\r\n",
        "        # with every other training example, don't be confused by einsum\r\n",
        "        # it's just how I like doing matrix multiplication & bmm\r\n",
        "\r\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\r\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\r\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\r\n",
        "        # energy: (N, heads, query_len, key_len)\r\n",
        "\r\n",
        "        # Mask padded indices so their weights become 0\r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\r\n",
        "\r\n",
        "        # Normalize energy values similarly to seq2seq + attention\r\n",
        "        # so that they sum to 1. Also divide by scaling factor for\r\n",
        "        # better stability\r\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\r\n",
        "        # attention shape: (N, heads, query_len, key_len)\r\n",
        "\r\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\r\n",
        "            N, query_len, self.heads * self.head_dim\r\n",
        "        )\r\n",
        "        # attention shape: (N, heads, query_len, key_len)\r\n",
        "        # values shape: (N, value_len, heads, heads_dim)\r\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\r\n",
        "        # we reshape and flatten the last two dimensions.\r\n",
        "\r\n",
        "        out = self.att_fc(out)\r\n",
        "        # Linear layer doesn't modify the shape, final shape will be\r\n",
        "        # (N, query_len, embed_size)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class TransformerBlock(nn.Module):\r\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\r\n",
        "        super(TransformerBlock, self).__init__()\r\n",
        "        self.attention = SelfAttention(embed_size, heads)\r\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\r\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\r\n",
        "\r\n",
        "        self.trb_fc1 = nn.Linear(embed_size, forward_expansion * embed_size)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.trb_fc2 = nn.Linear(forward_expansion * embed_size, embed_size)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, value, key, query, mask):\r\n",
        "        attention = self.attention(value, key, query, mask)\r\n",
        "\r\n",
        "        # Add skip connection, run through normalization and finally dropout\r\n",
        "        x = self.dropout(self.norm1(attention + query))\r\n",
        "        forward = self.trb_fc1(x)\r\n",
        "        forward = self.relu(forward)\r\n",
        "        forward = self.trb_fc2(forward)\r\n",
        "        out = self.dropout(self.norm2(forward + x))\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        embed_size,\r\n",
        "        num_layers,\r\n",
        "        forward_expansion,\r\n",
        "        heads,\r\n",
        "        dropout,\r\n",
        "        vocab_size,\r\n",
        "        max_length\r\n",
        "    ):\r\n",
        "\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.embed_size = embed_size\r\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList(\r\n",
        "            [\r\n",
        "                TransformerBlock(\r\n",
        "                    embed_size,\r\n",
        "                    heads,\r\n",
        "                    dropout=dropout,\r\n",
        "                    forward_expansion=forward_expansion,\r\n",
        "                )\r\n",
        "                for _ in range(num_layers)\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, mask):\r\n",
        "        N, seq_length = x.shape\r\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\r\n",
        "        out = self.dropout(\r\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\r\n",
        "        )\r\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\r\n",
        "        # decoder this will change. This might look a bit odd in this case.\r\n",
        "        for layer in self.layers:\r\n",
        "            out = layer(out, out, out, mask)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class DecoderBlock(nn.Module):\r\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout):\r\n",
        "        super(DecoderBlock, self).__init__()\r\n",
        "        self.norm = nn.LayerNorm(embed_size)\r\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\r\n",
        "        self.transformer_block = TransformerBlock(\r\n",
        "            embed_size, heads, dropout, forward_expansion\r\n",
        "        )\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\r\n",
        "        attention = self.attention(x, x, x, trg_mask)\r\n",
        "        query = self.dropout(self.norm(attention + x))\r\n",
        "        out = self.transformer_block(value, key, query, src_mask)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        embed_size,\r\n",
        "        num_layers,\r\n",
        "        forward_expansion,\r\n",
        "        heads,\r\n",
        "        dropout,\r\n",
        "        vocab_size,\r\n",
        "        max_length\r\n",
        "    ):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.embed_size = embed_size\r\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList(\r\n",
        "            [\r\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout)\r\n",
        "                for _ in range(num_layers)\r\n",
        "            ]\r\n",
        "        )\r\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\r\n",
        "        N, seq_length = x.shape\r\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\r\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\r\n",
        "\r\n",
        "        out = self.fc_out(x)\r\n",
        "\r\n",
        "        return F.log_softmax(out, dim=-1)\r\n",
        "\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        embed_size=768,\r\n",
        "        num_layers=1,\r\n",
        "        forward_expansion=4,\r\n",
        "        heads=12,\r\n",
        "        dropout=0.1,\r\n",
        "        vocab_size=119547,\r\n",
        "        max_length=512,\r\n",
        "        pad_idx=0\r\n",
        "    ):\r\n",
        "\r\n",
        "        super(Transformer, self).__init__()\r\n",
        "\r\n",
        "        self.encoder = Encoder(\r\n",
        "            embed_size,\r\n",
        "            num_layers,\r\n",
        "            forward_expansion,\r\n",
        "            heads,\r\n",
        "            dropout,\r\n",
        "            vocab_size,\r\n",
        "            max_length\r\n",
        "        )\r\n",
        "\r\n",
        "        self.decoder = Decoder(\r\n",
        "            embed_size,\r\n",
        "            num_layers,\r\n",
        "            forward_expansion,\r\n",
        "            heads,\r\n",
        "            dropout,\r\n",
        "            vocab_size,\r\n",
        "            max_length\r\n",
        "        )\r\n",
        "\r\n",
        "        self.pad_idx = pad_idx\r\n",
        "\r\n",
        "    def make_src_mask(self, src):\r\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        # (N, 1, 1, src_len)\r\n",
        "        return src_mask.to(device)\r\n",
        "\r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        N, trg_len = trg.shape\r\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\r\n",
        "            N, 1, trg_len, trg_len\r\n",
        "        )\r\n",
        "\r\n",
        "        return trg_mask.to(device)\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaRc0CdIzgdx"
      },
      "source": [
        "\n",
        "class EWC(object):\n",
        "    \"\"\" Class implementing the Elastic Weight Consolidation approach described in http://arxiv.org/abs/1612.00796 \"\"\"\n",
        "\n",
        "    def __init__(self,model, lamb=4000, nepochs=100,sbatch=32,lr=0.001,lr_min=1e-6,lr_factor=3,lr_patience=5,name='model'):\n",
        "        self.model=model\n",
        "        self.model_old = deepcopy(self.get_param(self.model))\n",
        "        self.fisher=None\n",
        "        self.tasks = []\n",
        "\n",
        "        self.nepochs = nepochs\n",
        "        self.sbatch = sbatch\n",
        "        self.lr = lr\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_factor = lr_factor\n",
        "        self.lr_patience = lr_patience\n",
        "        self.lamb = lamb\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "\n",
        "    def add_task(self, src_token, trg_token):\n",
        "        if {'src': src_token, 'trg': trg_token} in self.tasks:\n",
        "            return\n",
        "        self.tasks.append({'src': src_token, 'trg': trg_token})\n",
        "        \n",
        "    def load_task(self, t):\n",
        "        path = '/content/drive/My Drive/Project3/trained_model/{}|{}'.format(self.name,\n",
        "                        '|'.join(['%s-%s'%(task['src'], task['trg']) for task in self.tasks[:t]]))\n",
        "        self.model.load_state_dict(torch.load('%s.pt'%path))\n",
        "        self.model.to(device)\n",
        "        self.fisher = torch.load('%s_fisher.pt'%path)\n",
        "        self.model_old = deepcopy(self.get_param(self.model))\n",
        "\n",
        "    def get_param(self, model):\n",
        "        sd = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'word_embedding' in name or 'position_embedding' in name or 'norm' in name or 'norm1' in name or 'norm2' in name:\n",
        "                continue \n",
        "            sd.append((name, param))\n",
        "        return sd\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        # return torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n",
        "   \n",
        "    def train(self, src_train, trg_train, src_valid, trg_valid):\n",
        "        best_loss = np.inf\n",
        "        lr = self.lr\n",
        "        patience = self.lr_patience\n",
        "        self.ce=torch.nn.CrossEntropyLoss(ignore_index=self.model.pad_idx)\n",
        "        self.optimizer=self.get_optimizer()\n",
        "        path = '/content/drive/My Drive/Project3/trained_model/{}|{}'.format(self.name,\n",
        "                        '|'.join(['%s-%s'%(task['src'], task['trg']) for task in self.tasks]))\n",
        "        # Loop epochs\n",
        "        for e in range(self.nepochs):\n",
        "            try:\n",
        "                # Train\n",
        "                clock0=time.time()\n",
        "                train_loss = self.train_epoch(src_train, trg_train)\n",
        "                clock1=time.time()\n",
        "                valid_loss = self.eval(src_valid, trg_valid)\n",
        "                clock2=time.time()\n",
        "                print('| Epoch {:3d}, time={:5.1f}ms/{:5.1f}ms | Train: loss={:.3f} |'.format(\n",
        "                    e+1,1000*(clock1-clock0),\n",
        "                    1000*(clock2-clock1),train_loss),end='')\n",
        "                # # Valid\n",
        "                print(' Valid: loss={:.3f} |'.format(valid_loss),end='')\n",
        "                \n",
        "                # Adapt lr\n",
        "                if valid_loss < best_loss:\n",
        "                    best_loss = valid_loss\n",
        "                    torch.save(self.model.state_dict(),'%s.pt' % path)\n",
        "                    patience = self.lr_patience\n",
        "                    print(' *', end='')\n",
        "                \n",
        "                else:\n",
        "                    patience -= 1\n",
        "                    if patience <= 0:\n",
        "                        lr /= self.lr_factor\n",
        "                        print(' lr={:.1e}'.format(lr), end='')\n",
        "                        if lr < self.lr_min:\n",
        "                            print()\n",
        "                            break\n",
        "                            \n",
        "                        patience = self.lr_patience\n",
        "                        self.optimizer = self.get_optimizer()\n",
        "                print()\n",
        "            except KeyboardInterrupt:\n",
        "                break\n",
        "\n",
        "        # Restore best\n",
        "        # torch.save(self.model.state_dict(),'%s.pt' % path)\n",
        "        print('computing fisher ...')\n",
        "        self.model.load_state_dict(torch.load('%s.pt'%path))\n",
        "        self.model_old = deepcopy(self.get_param(self.model))\n",
        "        \n",
        "        # Fisher ops\n",
        "        if len(self.tasks) > 1:\n",
        "            fisher_old={}\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                fisher_old[n]=self.fisher[n].clone()\n",
        "        self.fisher=self.fisher_matrix_diag(src_train, trg_train)\n",
        "        if len(self.tasks) > 1:\n",
        "            t = len(self.tasks)-1\n",
        "            # Watch out! We do not want to keep t models (or fisher diagonals) in memory, therefore we have to merge fisher diagonals\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                self.fisher[n]=(self.fisher[n]+fisher_old[n]*t)/(t+1)       # Checked: it is better than the other option\n",
        "                #self.fisher[n]=0.5*(self.fisher[n]+fisher_old[n])\n",
        "\n",
        "        torch.save(self.fisher,'%s_fisher.pt'%path)\n",
        "        return\n",
        "\n",
        "    def train_epoch(self, src_data, trg_data):\n",
        "        self.model.train()\n",
        "        total_loss=0\n",
        "        idx = np.random.permutation(src_data.shape[0])\n",
        "        num_iters = src_data.shape[0]//self.sbatch+1\n",
        "        for i in range(num_iters):\n",
        "            top = min((i+1)*self.sbatch, src_data.shape[0])\n",
        "            src = src_data[idx[i*self.sbatch:top]].to(device)\n",
        "            trg = trg_data[idx[i*self.sbatch:top]].to(device)\n",
        "            output = self.model(src, trg[:, :-1])\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)   \n",
        "            loss=self.criterion(output,trg)\n",
        "            total_loss += loss.item()\n",
        "            # Backward\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return total_loss/num_iters\n",
        "\n",
        "    def eval(self, src_data, trg_data):\n",
        "        total_loss=0\n",
        "        self.model.eval()\n",
        "        idx = np.random.permutation(src_data.shape[0])\n",
        "        num_iters = src_data.shape[0]//self.sbatch+1\n",
        "        for i in range(num_iters):\n",
        "            top = min((i+1)*self.sbatch, src_data.shape[0])\n",
        "            src = src_data[idx[i*self.sbatch:top]].to(device)\n",
        "            trg = trg_data[idx[i*self.sbatch:top]].to(device)\n",
        "            output = self.model(src, trg[:, :-1])\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)   \n",
        "            loss=self.criterion(output,trg)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss/num_iters\n",
        "\n",
        "    def criterion(self,output,targets):\n",
        "        # Regularization for all previous tasks\n",
        "        loss_reg=0\n",
        "        if len(self.tasks) > 1:\n",
        "            for (name,param),(_,param_old) in zip(self.get_param(self.model),self.model_old):   \n",
        "                # print(self.fisher[name].shape, param.shape, param_old.shape)\n",
        "                loss_reg+=torch.sum(self.fisher[name]*(param_old-param).pow(2))/2\n",
        "        return self.ce(output,targets)+self.lamb*loss_reg\n",
        "\n",
        "    def fisher_matrix_diag(self, src_data, trg_data):\n",
        "        # Init\n",
        "        fisher={}\n",
        "        for n,p in self.get_param(self.model):\n",
        "            fisher[n]=0*p.data\n",
        "        # Compute\n",
        "        self.model.train()\n",
        "        idx = np.random.permutation(src_data.shape[0])\n",
        "        num_iters = src_data.shape[0]//self.sbatch+1\n",
        "        for i in range(num_iters):\n",
        "            top = min((i+1)*self.sbatch, src_data.shape[0])\n",
        "            src = src_data[idx[i*self.sbatch:top]].to(device)\n",
        "            trg = trg_data[idx[i*self.sbatch:top]].to(device)\n",
        "            output = self.model(src, trg[:, :-1])\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            trg = trg[:,1:].reshape(-1)\n",
        "            loss=self.criterion(output,trg)\n",
        "            loss.backward()\n",
        "            # Get gradients\n",
        "            for n,p in self.get_param(self.model):\n",
        "                if p.grad is not None:\n",
        "                    fisher[n]+=src.shape[0]*p.grad.data.pow(2)\n",
        "        # Mean\n",
        "        with torch.no_grad():\n",
        "            for n,_ in self.get_param(self.model):\n",
        "                fisher[n]=fisher[n]/num_iters\n",
        "        return fisher\n",
        "\n",
        "def translate(model, src_sen, max_length=512):\n",
        "\n",
        "    model.eval()\n",
        "    src_tensor = tokenizer(src_sen, return_tensors='pt')['input_ids'].to(device)\n",
        "    outputs = [tokenizer.cls_token_id]\n",
        "    for i in range(max_length):\n",
        "        translate_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, translate_tensor)\n",
        "        \n",
        "        best_guess = output.argmax(2)[:,-1].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "    outputs = outputs[1:-1]\n",
        "    translated_sentence = tokenizer.decode(outputs)\n",
        "    return translated_sentence\n",
        "\n",
        "def eval_bleu_score(model, src_data, trg_data):\n",
        "    total_bleu = 0\n",
        "    for src_sen, trg_sen in zip(src_data, trg_data):\n",
        "        translated_sentence = translate(model, src_sen)\n",
        "        bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split()], translated_sentence.split(), weights=(0.5, 0.5))\n",
        "        total_bleu += bleu_score\n",
        "    return total_bleu/len(src_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G16urIQlQRM"
      },
      "source": [
        "model = Transformer(\r\n",
        "    embed_size=768,\r\n",
        "    num_layers=1,\r\n",
        "    forward_expansion=4,\r\n",
        "    heads=12,\r\n",
        "    dropout=0.1,\r\n",
        "    vocab_size=tokenizer.vocab_size,\r\n",
        "    max_length=512,\r\n",
        "    pad_idx=0\r\n",
        ").to(device)\r\n",
        "\r\n",
        "ewc = EWC(\r\n",
        "    model=model,\r\n",
        "    lamb=4000,\r\n",
        "    nepochs=20,\r\n",
        "    sbatch=32,\r\n",
        "    lr=0.001,\r\n",
        "    lr_min=1e-6,\r\n",
        "    lr_factor=3,\r\n",
        "    lr_patience=5,\r\n",
        "    name='mult_bert'\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uye-zTSMTZPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b97563-1d9b-4785-ac6a-da464b1eb647"
      },
      "source": [
        "def preprocess(text):\n",
        "    # text = ViTokenizer.tokenize(text)\n",
        "    return ' '.join(re.split('\\W+', text)).lower()\n",
        "\n",
        "def load_data(path):\n",
        "    with open('/content/drive/MyDrive/Project3/data/%s'%path, encoding='utf-8') as f:\n",
        "        data = [line.split('\\t')[:2] for line in f.read().splitlines()]\n",
        "\n",
        "    src = [preprocess(line[0]) for line in data]\n",
        "    trg = [preprocess(line[1]) for line in data]\n",
        "    return src, trg\n",
        "\n",
        "eng_train, viet_train = load_data('eng-viet_train.txt')\n",
        "eng_train_tensor = tokenizer(eng_train, return_tensors='pt', padding=True)['input_ids']\n",
        "viet_train_tensor = tokenizer(viet_train, return_tensors='pt', padding=True)['input_ids']\n",
        "\n",
        "eng_valid, viet_valid = load_data('eng-viet_valid.txt')\n",
        "eng_valid_tensor = tokenizer(eng_valid, return_tensors='pt', padding=True)['input_ids']\n",
        "viet_valid_tensor = tokenizer(viet_valid, return_tensors='pt', padding=True)['input_ids']\n",
        "\n",
        "eng_test, viet_test = load_data('eng-viet_test.txt')\n",
        "\n",
        "print('train:', len(eng_train))\n",
        "print('valid:', len(eng_valid))\n",
        "print('test:', len(eng_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 2949\n",
            "valid: 521\n",
            "test: 613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjQcVNAwaxC1",
        "outputId": "41eea7ef-f84b-4459-cf80-630f098bcc11"
      },
      "source": [
        "ewc.add_task('viet','eng')\r\n",
        "ewc.train(viet_train_tensor, eng_train_tensor, viet_valid_tensor, eng_valid_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=31969.9ms/1239.3ms | Train: loss=6.026 | Valid: loss=4.980 | *\n",
            "| Epoch   2, time=32349.6ms/1272.9ms | Train: loss=4.092 | Valid: loss=4.195 | *\n",
            "| Epoch   3, time=32885.3ms/1315.2ms | Train: loss=3.024 | Valid: loss=3.886 | *\n",
            "| Epoch   4, time=33351.3ms/1353.5ms | Train: loss=2.179 | Valid: loss=3.711 | *\n",
            "| Epoch   5, time=33945.7ms/1388.3ms | Train: loss=1.501 | Valid: loss=3.619 | *\n",
            "| Epoch   6, time=34466.8ms/1403.0ms | Train: loss=0.967 | Valid: loss=3.740 |\n",
            "| Epoch   7, time=34592.6ms/1403.1ms | Train: loss=0.638 | Valid: loss=3.722 |\n",
            "| Epoch   8, time=34460.6ms/1413.0ms | Train: loss=0.435 | Valid: loss=3.884 |\n",
            "| Epoch   9, time=34605.7ms/1412.5ms | Train: loss=0.297 | Valid: loss=3.910 |\n",
            "| Epoch  10, time=34543.2ms/1396.0ms | Train: loss=0.223 | Valid: loss=4.016 | lr=3.3e-04\n",
            "| Epoch  11, time=34513.8ms/1405.9ms | Train: loss=0.357 | Valid: loss=4.034 |\n",
            "| Epoch  12, time=34504.2ms/1407.1ms | Train: loss=0.241 | Valid: loss=4.203 |\n",
            "| Epoch  13, time=34495.4ms/1401.1ms | Train: loss=0.172 | Valid: loss=4.362 |\n",
            "| Epoch  14, time=34522.0ms/1398.8ms | Train: loss=0.159 | Valid: loss=4.518 |\n",
            "| Epoch  15, time=34505.7ms/1400.3ms | Train: loss=0.169 | Valid: loss=4.478 | lr=1.1e-04\n",
            "| Epoch  16, time=34542.9ms/1416.9ms | Train: loss=0.220 | Valid: loss=4.429 |\n",
            "| Epoch  17, time=34476.0ms/1409.6ms | Train: loss=0.163 | Valid: loss=4.546 |\n",
            "| Epoch  18, time=34515.7ms/1405.6ms | Train: loss=0.148 | Valid: loss=4.744 |\n",
            "| Epoch  19, time=34479.4ms/1396.8ms | Train: loss=0.150 | Valid: loss=4.814 |\n",
            "| Epoch  20, time=34497.8ms/1397.4ms | Train: loss=0.144 | Valid: loss=4.810 | lr=3.7e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfoow7i3aBfP",
        "outputId": "2e8b17f5-b06c-448b-96cf-c3ee2078a1a4"
      },
      "source": [
        "task1_bleu = eval_bleu_score(model, viet_test, eng_test)\r\n",
        "print('task 1, test viet-eng:', task1_bleu)\r\n",
        "task1_bleu = eval_bleu_score(model, viet_valid, eng_valid)\r\n",
        "print('task 1, valid viet-eng:', task1_bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1, test viet-eng: 0.3998202204324897\n",
            "task 1, valid viet-eng: 0.39602111879803276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYPleIZYjIgb",
        "outputId": "cac9600a-8dbd-4594-eb1e-2c4ee72ad9ce"
      },
      "source": [
        "eng1_train, fra_train = load_data('eng-fra_train.txt')\r\n",
        "eng1_train, fra_train = eng1_train[:5000], fra_train[:5000]\r\n",
        "eng1_train_tensor = tokenizer(eng1_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "fra_train_tensor = tokenizer(fra_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng1_valid, fra_valid = load_data('eng-fra_valid.txt')\r\n",
        "eng1_valid, fra_valid = eng1_valid[:600], fra_valid[:600]\r\n",
        "eng1_valid_tensor = tokenizer(eng1_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "fra_valid_tensor = tokenizer(fra_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng1_test, fra_test = load_data('eng-fra_test.txt')\r\n",
        "eng1_test, fra_test = eng1_test[:700], fra_test[:700]\r\n",
        "\r\n",
        "print('train:', len(eng1_train))\r\n",
        "print('valid:', len(eng1_valid))\r\n",
        "print('test:', len(eng1_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 5000\n",
            "valid: 600\n",
            "test: 700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp-A52SkikDh",
        "outputId": "84deb07c-d8af-4bee-df09-a0fe28ad7c82"
      },
      "source": [
        "ewc.add_task('fra','eng')\r\n",
        "ewc.train(fra_train_tensor, eng1_train_tensor, fra_valid_tensor, eng1_valid_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=65875.4ms/2057.6ms | Train: loss=8.423 | Valid: loss=4.952 | *\n",
            "| Epoch   2, time=65891.3ms/2032.8ms | Train: loss=4.639 | Valid: loss=4.792 | *\n",
            "| Epoch   3, time=65987.8ms/2046.4ms | Train: loss=4.237 | Valid: loss=4.719 | *\n",
            "| Epoch   4, time=65940.9ms/2050.7ms | Train: loss=3.918 | Valid: loss=4.747 |\n",
            "| Epoch   5, time=65822.7ms/2095.8ms | Train: loss=3.640 | Valid: loss=4.681 | *\n",
            "| Epoch   6, time=66089.7ms/2085.7ms | Train: loss=3.418 | Valid: loss=4.680 | *\n",
            "| Epoch   7, time=65993.3ms/2058.3ms | Train: loss=3.229 | Valid: loss=4.652 | *\n",
            "| Epoch   8, time=66054.4ms/2066.5ms | Train: loss=3.070 | Valid: loss=4.674 |\n",
            "| Epoch   9, time=66024.0ms/2096.5ms | Train: loss=2.939 | Valid: loss=4.635 | *\n",
            "| Epoch  10, time=66100.8ms/2074.6ms | Train: loss=2.801 | Valid: loss=4.743 |\n",
            "| Epoch  11, time=65986.9ms/2100.5ms | Train: loss=2.705 | Valid: loss=4.770 |\n",
            "| Epoch  12, time=65924.5ms/2093.0ms | Train: loss=2.601 | Valid: loss=4.759 |\n",
            "| Epoch  13, time=65943.3ms/2089.3ms | Train: loss=2.534 | Valid: loss=4.890 |\n",
            "| Epoch  14, time=65914.4ms/2084.5ms | Train: loss=2.466 | Valid: loss=4.935 | lr=3.3e-04\n",
            "| Epoch  15, time=65943.9ms/2110.1ms | Train: loss=4.798 | Valid: loss=4.714 |\n",
            "| Epoch  16, time=65956.4ms/2092.5ms | Train: loss=2.188 | Valid: loss=4.826 |\n",
            "| Epoch  17, time=65864.9ms/2092.9ms | Train: loss=2.118 | Valid: loss=4.889 |\n",
            "| Epoch  18, time=65910.7ms/2098.9ms | Train: loss=2.042 | Valid: loss=4.995 |\n",
            "| Epoch  19, time=65968.8ms/2089.7ms | Train: loss=1.991 | Valid: loss=5.093 | lr=1.1e-04\n",
            "| Epoch  20, time=65946.1ms/2094.0ms | Train: loss=4.535 | Valid: loss=4.959 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H1GByRwaTsx",
        "outputId": "b6edb4d2-8024-4057-ca20-51b4bd0d899a"
      },
      "source": [
        "task1_bleu = eval_bleu_score(model, viet_test, eng_test)\r\n",
        "print('task 1, test viet-eng:', task1_bleu)\r\n",
        "task1_bleu = eval_bleu_score(model, viet_valid, eng_valid)\r\n",
        "print('task 1, valid viet-eng:', task1_bleu)\r\n",
        "\r\n",
        "task2_bleu = eval_bleu_score(model, fra_test, eng1_test)\r\n",
        "print('task 2, test fra-eng:', task2_bleu)\r\n",
        "task2_bleu = eval_bleu_score(model, fra_valid, eng1_valid)\r\n",
        "print('task 2, valid fra-eng:', task2_bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1, test viet-eng: 0.37303431428479344\n",
            "task 1, valid viet-eng: 0.3829520065195675\n",
            "task 2, test fra-eng: 0.43069504524350344\n",
            "task 2, valid fra-eng: 0.413542653300227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go9D5eiv9ie9",
        "outputId": "767e08de-773b-4015-ac2d-986128bed7a8"
      },
      "source": [
        "eng2_train, deu_train = load_data('eng-deu_train.txt')\r\n",
        "eng2_train, deu_train = eng2_train[:5000], deu_train[:5000]\r\n",
        "eng2_train_tensor = tokenizer(eng2_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "deu_train_tensor = tokenizer(deu_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng2_valid, deu_valid = load_data('eng-deu_valid.txt')\r\n",
        "eng2_valid, deu_valid = eng2_valid[:600], deu_valid[:600]\r\n",
        "eng2_valid_tensor = tokenizer(eng2_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "deu_valid_tensor = tokenizer(deu_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng2_test, deu_test = load_data('eng-deu_test.txt')\r\n",
        "eng2_test, deu_test = eng2_test[:700], deu_test[:700]\r\n",
        "\r\n",
        "print('train:', len(eng2_train))\r\n",
        "print('valid:', len(eng2_valid))\r\n",
        "print('test:', len(eng2_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 5000\n",
            "valid: 600\n",
            "test: 700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dawXpg_98phx",
        "outputId": "0d15dc7f-d5c8-41b2-e1b3-eb2519602304"
      },
      "source": [
        "ewc.add_task('deu','eng')\r\n",
        "ewc.train(deu_train_tensor, eng2_train_tensor, deu_valid_tensor, eng2_valid_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=59841.8ms/1798.7ms | Train: loss=8.753 | Valid: loss=5.250 | *\n",
            "| Epoch   2, time=59690.3ms/1806.1ms | Train: loss=4.834 | Valid: loss=4.953 | *\n",
            "| Epoch   3, time=59503.6ms/1804.7ms | Train: loss=4.525 | Valid: loss=4.948 | *\n",
            "| Epoch   4, time=59645.7ms/1804.1ms | Train: loss=4.333 | Valid: loss=4.895 | *\n",
            "| Epoch   5, time=59672.1ms/1803.8ms | Train: loss=4.164 | Valid: loss=4.935 |\n",
            "| Epoch   6, time=59297.3ms/1830.9ms | Train: loss=4.050 | Valid: loss=4.880 | *\n",
            "| Epoch   7, time=59592.8ms/1824.8ms | Train: loss=3.940 | Valid: loss=4.951 |\n",
            "| Epoch   8, time=59332.0ms/1823.5ms | Train: loss=3.860 | Valid: loss=4.880 | *\n",
            "| Epoch   9, time=59636.9ms/1808.5ms | Train: loss=3.786 | Valid: loss=4.920 |\n",
            "| Epoch  10, time=59363.7ms/1836.4ms | Train: loss=3.707 | Valid: loss=4.915 |\n",
            "| Epoch  11, time=59350.8ms/1831.7ms | Train: loss=3.640 | Valid: loss=4.874 | *\n",
            "| Epoch  12, time=59621.0ms/1804.4ms | Train: loss=3.581 | Valid: loss=4.926 |\n",
            "| Epoch  13, time=59326.6ms/1829.1ms | Train: loss=3.550 | Valid: loss=4.932 |\n",
            "| Epoch  14, time=59291.6ms/1821.5ms | Train: loss=3.514 | Valid: loss=5.028 |\n",
            "| Epoch  15, time=59302.6ms/1818.8ms | Train: loss=3.470 | Valid: loss=4.950 |\n",
            "| Epoch  16, time=59350.1ms/1826.2ms | Train: loss=3.399 | Valid: loss=4.996 | lr=3.3e-04\n",
            "| Epoch  17, time=59362.6ms/1830.1ms | Train: loss=5.892 | Valid: loss=4.783 | *\n",
            "| Epoch  18, time=59642.3ms/1802.8ms | Train: loss=3.210 | Valid: loss=4.828 |\n",
            "| Epoch  19, time=59461.7ms/1831.3ms | Train: loss=3.170 | Valid: loss=4.865 |\n",
            "| Epoch  20, time=59336.1ms/1824.3ms | Train: loss=3.135 | Valid: loss=4.920 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVR1tJZqVofn",
        "outputId": "c1be55d9-8283-4804-b679-7e15c830ca39"
      },
      "source": [
        "task1_bleu = eval_bleu_score(model, viet_test, eng_test)\r\n",
        "print('task 1, test viet-eng:', task1_bleu)\r\n",
        "task1_bleu = eval_bleu_score(model, viet_valid, eng_valid)\r\n",
        "print('task 1, valid viet-eng:', task1_bleu)\r\n",
        "\r\n",
        "task2_bleu = eval_bleu_score(model, fra_test, eng1_test)\r\n",
        "print('task 2, test fra-eng:', task2_bleu)\r\n",
        "task2_bleu = eval_bleu_score(model, fra_valid, eng1_valid)\r\n",
        "print('task 2, valid fra-eng:', task2_bleu)\r\n",
        "\r\n",
        "task3_bleu = eval_bleu_score(model, deu_test, eng2_test)\r\n",
        "print('task 3, test deu-eng:', task3_bleu)\r\n",
        "task3_bleu = eval_bleu_score(model, deu_valid, eng2_valid)\r\n",
        "print('task 3, valid deu-eng:', task3_bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1, test viet-eng: 0.3503040176663406\n",
            "task 1, valid viet-eng: 0.35973018894940473\n",
            "task 2, test fra-eng: 0.3954532992003429\n",
            "task 2, valid fra-eng: 0.3835493491881809\n",
            "task 3, test deu-eng: 0.4054799232042307\n",
            "task 3, valid deu-eng: 0.3906546974230925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmzGPR6TC6J5",
        "outputId": "0dadbfc7-daae-48e9-d419-1416feafef3f"
      },
      "source": [
        "eng3_train, cmn_train = load_data('eng-cmn_train.txt')\r\n",
        "eng3_train, cmn_train = eng3_train[:5000], cmn_train[:5000]\r\n",
        "eng3_train_tensor = tokenizer(eng3_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "cmn_train_tensor = tokenizer(cmn_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng3_valid, cmn_valid = load_data('eng-cmn_valid.txt')\r\n",
        "eng3_valid, cmn_valid = eng3_valid[:600], cmn_valid[:600]\r\n",
        "eng3_valid_tensor = tokenizer(eng3_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "cmn_valid_tensor = tokenizer(cmn_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng3_test, cmn_test = load_data('eng-cmn_test.txt')\r\n",
        "eng3_test, cmn_test = eng3_test[:700], cmn_test[:700]\r\n",
        "\r\n",
        "print('train:', len(eng3_train))\r\n",
        "print('valid:', len(eng3_valid))\r\n",
        "print('test:', len(eng3_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 5000\n",
            "valid: 600\n",
            "test: 700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV_QybWD-kYx",
        "outputId": "66449408-d704-42b0-a90f-ce14521b279b"
      },
      "source": [
        "ewc.add_task('cmn','eng')\r\n",
        "ewc.train(cmn_train_tensor, eng3_train_tensor, cmn_valid_tensor, eng3_valid_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   1, time=52623.1ms/1721.3ms | Train: loss=10.103 | Valid: loss=5.305 | *\n",
            "| Epoch   2, time=53195.8ms/1659.0ms | Train: loss=5.064 | Valid: loss=5.055 | *\n",
            "| Epoch   3, time=53292.9ms/1680.8ms | Train: loss=4.700 | Valid: loss=4.817 | *\n",
            "| Epoch   4, time=53196.2ms/1693.4ms | Train: loss=4.530 | Valid: loss=4.842 |\n",
            "| Epoch   5, time=53084.6ms/1671.1ms | Train: loss=4.389 | Valid: loss=4.758 | *\n",
            "| Epoch   6, time=53243.0ms/1677.2ms | Train: loss=4.295 | Valid: loss=4.750 | *\n",
            "| Epoch   7, time=53208.0ms/1690.6ms | Train: loss=4.198 | Valid: loss=4.732 | *\n",
            "| Epoch   8, time=53184.3ms/1686.4ms | Train: loss=4.131 | Valid: loss=4.712 | *\n",
            "| Epoch   9, time=53269.5ms/1676.5ms | Train: loss=4.064 | Valid: loss=4.790 |\n",
            "| Epoch  10, time=53059.7ms/1679.1ms | Train: loss=4.014 | Valid: loss=4.714 |\n",
            "| Epoch  11, time=53160.3ms/1684.7ms | Train: loss=3.950 | Valid: loss=4.716 |\n",
            "| Epoch  12, time=53162.9ms/1681.4ms | Train: loss=3.906 | Valid: loss=4.674 | *\n",
            "| Epoch  13, time=53170.2ms/1671.0ms | Train: loss=3.873 | Valid: loss=4.761 |\n",
            "| Epoch  14, time=53098.2ms/1690.7ms | Train: loss=3.858 | Valid: loss=4.796 |\n",
            "| Epoch  15, time=53104.9ms/1667.1ms | Train: loss=3.819 | Valid: loss=4.774 |\n",
            "| Epoch  16, time=53090.7ms/1689.5ms | Train: loss=3.776 | Valid: loss=4.739 |\n",
            "| Epoch  17, time=53072.3ms/1675.7ms | Train: loss=3.727 | Valid: loss=4.708 | lr=3.3e-04\n",
            "| Epoch  18, time=53114.1ms/1692.0ms | Train: loss=7.428 | Valid: loss=4.449 | *\n",
            "| Epoch  19, time=53212.1ms/1672.6ms | Train: loss=3.466 | Valid: loss=4.503 |\n",
            "| Epoch  20, time=53045.8ms/1682.0ms | Train: loss=3.447 | Valid: loss=4.536 |\n",
            "computing fisher ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evxhlEWtMfPN",
        "outputId": "443d08c2-07d0-4eee-9abf-3b54bdc1b778"
      },
      "source": [
        "task1_bleu = eval_bleu_score(model, viet_test, eng_test)\r\n",
        "print('task 1, test viet-eng:', task1_bleu)\r\n",
        "task1_bleu = eval_bleu_score(model, viet_valid, eng_valid)\r\n",
        "print('task 1, valid viet-eng:', task1_bleu)\r\n",
        "\r\n",
        "task2_bleu = eval_bleu_score(model, fra_test, eng1_test)\r\n",
        "print('task 2, test fra-eng:', task2_bleu)\r\n",
        "task2_bleu = eval_bleu_score(model, fra_valid, eng1_valid)\r\n",
        "print('task 2, valid fra-eng:', task2_bleu)\r\n",
        "\r\n",
        "task3_bleu = eval_bleu_score(model, deu_test, eng2_test)\r\n",
        "print('task 3, test deu-eng:', task3_bleu)\r\n",
        "task3_bleu = eval_bleu_score(model, deu_valid, eng2_valid)\r\n",
        "print('task 3, valid deu-eng:', task3_bleu)\r\n",
        "\r\n",
        "task4_bleu = eval_bleu_score(model, cmn_test, eng3_test)\r\n",
        "print('task 4, test cmn-eng:', task4_bleu)\r\n",
        "task4_bleu = eval_bleu_score(model, cmn_valid, eng3_valid)\r\n",
        "print('task 4, valid cmn-eng:', task4_bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 1, test viet-eng: 0.35527132347256246\n",
            "task 1, valid viet-eng: 0.35280394367011453\n",
            "task 2, test fra-eng: 0.38236695900386225\n",
            "task 2, valid fra-eng: 0.371775317150316\n",
            "task 3, test deu-eng: 0.38650534554786103\n",
            "task 3, valid deu-eng: 0.3810107931110714\n",
            "task 4, test cmn-eng: 0.3781235031608796\n",
            "task 4, valid cmn-eng: 0.36552328419280294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgwYW1bgRZIW",
        "outputId": "ff93950a-1431-4119-e8c7-bc417e163c9c"
      },
      "source": [
        "task4_bleu = eval_bleu_score(model, cmn_test, eng3_test)\r\n",
        "print('task 4, test cmn-eng:', task4_bleu)\r\n",
        "task4_bleu = eval_bleu_score(model, cmn_valid, eng3_valid)\r\n",
        "print('task 4, valid cmn-eng:', task4_bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task 4, test cmn-eng: 0.3566312436051986\n",
            "task 4, valid cmn-eng: 0.35761536666867244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW4crKnhhkwC",
        "outputId": "1641637d-974f-4208-91fb-003a8f7ef61f"
      },
      "source": [
        "for i in np.random.choice(len(viet_test), 10, replace=False):\r\n",
        "    src_sen = viet_test[i]\r\n",
        "    trg_sen = eng_test[i]\r\n",
        "    translated_sentence = translate(model, src_sen)\r\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split()], translated_sentence.split(), weights=(0.5, 0.5))\r\n",
        "    print()\r\n",
        "    print('> source sentence:', src_sen)\r\n",
        "    print('< translated sentence:', translated_sentence)\r\n",
        "    print('= correct sentence:', trg_sen)\r\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: bạn có muốn biết tôi đã làm điều đó như thế nào không \n",
            "< translated sentence: do you want to do anything but i want to do anything but a job\n",
            "= correct sentence: do you want to know how i did that \n",
            "bleu score: 0.26726124191242434\n",
            "\n",
            "> source sentence: tôi không thích đùa kiểu này \n",
            "< translated sentence: i don t like this way\n",
            "= correct sentence: i don t like these kinds of jokes \n",
            "bleu score: 0.45317419124773295\n",
            "\n",
            "> source sentence: tôi đã ăn đồ tối qua cho bữa trưa \n",
            "< translated sentence: i ate ice cream at the concert lasted\n",
            "= correct sentence: i ate last night s leftovers for lunch \n",
            "bleu score: 0.18898223650461357\n",
            "\n",
            "> source sentence: làm ơn phô tô thêm vài bản nhé \n",
            "< translated sentence: please don t waste of yourself\n",
            "= correct sentence: please have some copies made \n",
            "bleu score: 0.408248290463863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: tôi bị đau bụng \n",
            "< translated sentence: i m glad i m diet\n",
            "= correct sentence: i ve got an upset stomach \n",
            "bleu score: 0.408248290463863\n",
            "\n",
            "> source sentence: sao bạn không tới thăm tụi tôi \n",
            "< translated sentence: why don t you give me to get up\n",
            "= correct sentence: why don t you come visit us \n",
            "bleu score: 0.408248290463863\n",
            "\n",
            "> source sentence: nhà vua đã bị xử tử \n",
            "< translated sentence: home just arrived\n",
            "= correct sentence: the king was executed \n",
            "bleu score: 0\n",
            "\n",
            "> source sentence: cô ấy là một người nói tiếng anh thông thạo \n",
            "< translated sentence: she was a famous composer from english\n",
            "= correct sentence: she is a fluent speaker of english \n",
            "bleu score: 0.6546536707079771\n",
            "\n",
            "> source sentence: tôi ngủ có 2 tiếng à hèn chi bây giờ buồn ngủ ghê \n",
            "< translated sentence: i sleeps venture of the largest yokohas and four of the intent\n",
            "= correct sentence: i slept only two hours no wonder i m sleepy \n",
            "bleu score: 0.28867513459481287\n",
            "\n",
            "> source sentence: quyền lực mang đến tham nhũng \n",
            "< translated sentence: the police found the party\n",
            "= correct sentence: power brings corruption \n",
            "bleu score: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUShSVMT54aV",
        "outputId": "42b26a91-8c78-4527-e1e8-d4ce98b46bf9"
      },
      "source": [
        "for i in np.random.choice(len(eng1_test), 10, replace=False):\r\n",
        "    src_sen = fra_test[i]\r\n",
        "    trg_sen = eng1_test[i]\r\n",
        "    translated_sentence = translate(model, src_sen)\r\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split()], translated_sentence.split(), weights=(0.5, 0.5))\r\n",
        "    print()\r\n",
        "    print('> source sentence:', src_sen)\r\n",
        "    print('< translated sentence:', translated_sentence)\r\n",
        "    print('= correct sentence:', trg_sen)\r\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: je les ai laissées y aller \n",
            "< translated sentence: there are no one of the problem\n",
            "= correct sentence: i let them go \n",
            "bleu score: 0\n",
            "\n",
            "> source sentence: je les déteste tous \n",
            "< translated sentence: i love the whole world\n",
            "= correct sentence: i hate them all \n",
            "bleu score: 0.447213595499958\n",
            "\n",
            "> source sentence: ces chatons sont tellement mignons et câlins \n",
            "< translated sentence: these are engageds and physically these tickets are ninjas\n",
            "= correct sentence: these kittens are so cute and cuddly \n",
            "bleu score: 0.5773502691896257\n",
            "\n",
            "> source sentence: vous ne pourrez probablement pas faire cela \n",
            "< translated sentence: you don t make that for me to do that\n",
            "= correct sentence: you probably won t be able to do that \n",
            "bleu score: 0.3333333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: vous aurez besoin de cela \n",
            "< translated sentence: you re need to do that\n",
            "= correct sentence: you ll need that \n",
            "bleu score: 0.7071067811865476\n",
            "\n",
            "> source sentence: tom gémit bruyamment \n",
            "< translated sentence: tom complaineding the track\n",
            "= correct sentence: tom groaned loudly \n",
            "bleu score: 0.5\n",
            "\n",
            "> source sentence: je ne pensais pas que vous étiez si vieux \n",
            "< translated sentence: i didn t think you weren t you weren\n",
            "= correct sentence: i didn t think you were so old \n",
            "bleu score: 0.5270462766947299\n",
            "\n",
            "> source sentence: tom a adoré le film \n",
            "< translated sentence: tom has changed the telephone\n",
            "= correct sentence: tom enjoyed the movie \n",
            "bleu score: 0.6324555320336759\n",
            "\n",
            "> source sentence: tom réalisa qu il était seul \n",
            "< translated sentence: tom was built it was arrogant\n",
            "= correct sentence: tom realized he was alone \n",
            "bleu score: 0.5773502691896257\n",
            "\n",
            "> source sentence: je suis dépourvu d idées \n",
            "< translated sentence: i m sorry of my ideas\n",
            "= correct sentence: i m all out of ideas \n",
            "bleu score: 0.36514837167011077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE08stDVnbZL",
        "outputId": "9e5d4f60-6408-47f9-f6aa-56a8884021d1"
      },
      "source": [
        "for i in np.random.choice(len(deu_test), 10, replace=False):\r\n",
        "    src_sen = deu_test[i]\r\n",
        "    trg_sen = eng2_test[i]\r\n",
        "    translated_sentence = translate(model, src_sen)\r\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split()], translated_sentence.split(), weights=(0.5, 0.5))\r\n",
        "    print()\r\n",
        "    print('> source sentence:', src_sen)\r\n",
        "    print('< translated sentence:', translated_sentence)\r\n",
        "    print('= correct sentence:', trg_sen)\r\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: tom spielte krank \n",
            "< translated sentence: tom has a new car\n",
            "= correct sentence: tom pretended to be sick \n",
            "bleu score: 0.447213595499958\n",
            "\n",
            "> source sentence: es macht tom nicht allzu viel aus \n",
            "< translated sentence: tom doesn t have any longer\n",
            "= correct sentence: tom doesn t mind all that much \n",
            "bleu score: 0.3785581357133378\n",
            "\n",
            "> source sentence: ich kann tom nicht allein lassen \n",
            "< translated sentence: i can t let tom leave\n",
            "= correct sentence: i can t leave tom behind \n",
            "bleu score: 0.5773502691896258\n",
            "\n",
            "> source sentence: japan betreibt viel handel mit kanada \n",
            "< translated sentence: japan has a lot of nerve\n",
            "= correct sentence: japan does a lot of trade with canada \n",
            "bleu score: 0.3700151777184613\n",
            "\n",
            "> source sentence: ihnen bleibt nur noch sehr wenig zeit \n",
            "< translated sentence: it s very so much time\n",
            "= correct sentence: you have very little time left \n",
            "bleu score: 0.5773502691896257\n",
            "\n",
            "> source sentence: ich habe nach dir gesucht \n",
            "< translated sentence: i ve been waiting for you\n",
            "= correct sentence: i was looking for you \n",
            "bleu score: 0.316227766016838\n",
            "\n",
            "> source sentence: wie viele stunden schläfst du am tag \n",
            "< translated sentence: how many liters are you are in the dozen\n",
            "= correct sentence: how many hours a day do you sleep \n",
            "bleu score: 0.20412414523193154\n",
            "\n",
            "> source sentence: tom ist von seinem pferd gefallen \n",
            "< translated sentence: tom is his 60 his car\n",
            "= correct sentence: tom fell off his horse \n",
            "bleu score: 0.5773502691896257\n",
            "\n",
            "> source sentence: ich bin bereit alles zu tun um den verlust auszugleichen \n",
            "< translated sentence: i m going to do anything to get out of the way to get out\n",
            "= correct sentence: i am ready to do anything to make up for the loss \n",
            "bleu score: 0.29277002188455997\n",
            "\n",
            "> source sentence: bitte nicht aus der flasche trinken \n",
            "< translated sentence: please don t find the earthquake\n",
            "= correct sentence: don t drink from the bottle please \n",
            "bleu score: 0.3090914234923144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SPkWgb5BcxH",
        "outputId": "fa0d1b29-bf94-4864-a035-67d0dbb99e36"
      },
      "source": [
        "for i in np.random.choice(len(cmn_test), 10, replace=False):\r\n",
        "    src_sen = cmn_test[i]\r\n",
        "    trg_sen = eng3_test[i]\r\n",
        "    translated_sentence = translate(model, src_sen)\r\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([trg_sen.split()], translated_sentence.split(), weights=(0.5, 0.5))\r\n",
        "    print()\r\n",
        "    print('> source sentence:', src_sen)\r\n",
        "    print('< translated sentence:', translated_sentence)\r\n",
        "    print('= correct sentence:', trg_sen)\r\n",
        "    print('bleu score:',bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> source sentence: 一隻鳥正在樹上唱歌 \n",
            "< translated sentence: a new road is on the floor\n",
            "= correct sentence: a bird is singing in the tree \n",
            "bleu score: 0.6546536707079771\n",
            "\n",
            "> source sentence: 美國在1492年被哥倫布發現 \n",
            "< translated sentence: armenian is on the streets\n",
            "= correct sentence: america was discovered by columbus in 1492 \n",
            "bleu score: 0\n",
            "\n",
            "> source sentence: 那孩子沒有什麼朋友 \n",
            "< translated sentence: what scared you ve already begun\n",
            "= correct sentence: that child has few friends \n",
            "bleu score: 0\n",
            "\n",
            "> source sentence: 汤姆什么都没告诉你吗 \n",
            "< translated sentence: tom didn t know what you said\n",
            "= correct sentence: hasn t tom told you anything \n",
            "bleu score: 0.6546536707079771\n",
            "\n",
            "> source sentence: 我能回答所有的問題 \n",
            "< translated sentence: i can understand yourself\n",
            "= correct sentence: i could answer all the questions \n",
            "bleu score: 0.3032653298563167\n",
            "\n",
            "> source sentence: 我不完全肯定 \n",
            "< translated sentence: i m not sure to be a candidate\n",
            "= correct sentence: i m not absolutely sure \n",
            "bleu score: 0.3779644730092272\n",
            "\n",
            "> source sentence: 这不可能 \n",
            "< translated sentence: this is not can\n",
            "= correct sentence: that s impossible \n",
            "bleu score: 0\n",
            "\n",
            "> source sentence: 我们必须走了 \n",
            "< translated sentence: we have to get to the same\n",
            "= correct sentence: we have to go \n",
            "bleu score: 0.3779644730092272\n",
            "\n",
            "> source sentence: 你為什麼辭職 \n",
            "< translated sentence: what you think\n",
            "= correct sentence: why did you quit \n",
            "bleu score: 0.41368954504257255\n",
            "\n",
            "> source sentence: 當我醒來時 天正下著雪 \n",
            "< translated sentence: i ll leave forget to raining\n",
            "= correct sentence: i awoke to find it snowing \n",
            "bleu score: 0.5773502691896257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oz5CNvmCSWu"
      },
      "source": [
        "ewc.tasks=[]\r\n",
        "ewc.add_task('viet','eng')\r\n",
        "ewc.add_task('fra','eng')\r\n",
        "ewc.add_task('deu','eng')\r\n",
        "ewc.load_task(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9vifPAtbVe4",
        "outputId": "ab8bf09c-1f73-4a64-c7a0-7622b540b466"
      },
      "source": [
        "def preprocess(text):\r\n",
        "    # text = ViTokenizer.tokenize(text)\r\n",
        "    return ' '.join(re.split('\\W+', text)).lower()\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/train.en', encoding='utf-8') as f:\r\n",
        "    eng_train = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/train.vi', encoding='utf-8') as f:\r\n",
        "    viet_train = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/tst.en', encoding='utf-8') as f:\r\n",
        "    eng_test = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/tst.vi', encoding='utf-8') as f:\r\n",
        "    viet_test = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/dev.en', encoding='utf-8') as f:\r\n",
        "    eng_valid = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Project3/data/MT-EV-VLSP2020/indomain-news/dev.vi', encoding='utf-8') as f:\r\n",
        "    viet_valid = [preprocess(line) for line in f.read().splitlines()]\r\n",
        "\r\n",
        "eng_train_tensor = tokenizer(eng_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "viet_train_tensor = tokenizer(viet_train, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "eng_valid_tensor = tokenizer(eng_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "viet_valid_tensor = tokenizer(viet_valid, return_tensors='pt', padding=True)['input_ids']\r\n",
        "\r\n",
        "print('train:', len(eng_train))\r\n",
        "print('valid:', len(eng_valid))\r\n",
        "print('test:', len(eng_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 20000\n",
            "valid: 1007\n",
            "test: 1220\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}